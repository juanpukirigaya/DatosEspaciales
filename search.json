[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introducción al análisis de datos espaciales",
    "section": "",
    "text": "Presentación\nAquí se encontrarán herramientas para un análisis básico de datos espaciales.\nComo en muchas otras áreas de la estadística, se busca hacer el análisis en tres etapas:\n\nDescriptivo. Resumir visual y numéricamente los datos espaciales.\nIndicación. Explorar evidencia de estructuras espaciales o patrones.\nEstimación. Ajustar modelos estadísticos que permitan realizar inferencias o predicciones.",
    "crumbs": [
      "Presentación"
    ]
  },
  {
    "objectID": "Caps/01-Introduccion.html",
    "href": "Caps/01-Introduccion.html",
    "title": "1  Introducción",
    "section": "",
    "text": "1.1 Motivación\nGran parte del desarrollo clásico de la estadística se basa en la suposición de que las observaciones corresponden a realizaciones independientes e idénticamente distribuidas. Esta hipótesis es sensata en muestreos aleatorios o en experimentos controlados, y en particular implica que las observaciones no se influencían entre sí. La hipótesis es poco realista en muchos contextos donde las observaciones están influenciadas por su tiempo de ocurrencia, su ubicación en el espacio o alguna relación estructural propia del fenómeno.\nPara la incorporación de estructura espacial, podemos pensar en varias situaciones:\nLas herramientas con las que se cuenta hasta el momento son para preguntas del tipo quiero saber cómo se explica(n) la(s) propiedad(es) \\(Y\\) a partir de la(s) variable(s) \\(X\\), lo cual se planteó como un problema de regresión. Sin descartar esta herramienta, el considerar datos distribuidos en el espacio agrega una capa de complejidad al tener una estructura intrínseca a los fenómenos. Dicha estructura surge precisamente de considerar la ubicación del fenómeno y las condiciones de dicha ubicación. Los modelos espaciales surgen al reconocer que los datos están asociados a ubicaciones y que observaciones cercanas tienden a estar correlacionadas.\nPodemos considerar que entre las motivaciones principales para considerar modelos espaciales que muchos fenómenos reales como el clima, ecosistemas, epidemias, por mencionar algunos, no se comportan de manera independiente y la ubicación tiene un papel fundamental en su variabilidad. Basta pensar en que es más probable que sea parecida la temperatura entre dos puntos cercanos que entre dos puntos lejanos o que la prevalencia de una enfermedad dependa de factores ambientales o socioeconómicos.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "Caps/01-Introduccion.html#motivación",
    "href": "Caps/01-Introduccion.html#motivación",
    "title": "1  Introducción",
    "section": "",
    "text": "Los lugares donde se puede encontrar agua en un país.\nLa popularidad de cierto artista en distintos lugares del mundo.\nPlayas susceptibles a tener problemas por huracanes.\nLa distribución de distintos recursos naturales (como recursos mineros).\nLa calidad de vida de la población en determinado territorio.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "Caps/01-Introduccion.html#antecedentes-históricos",
    "href": "Caps/01-Introduccion.html#antecedentes-históricos",
    "title": "1  Introducción",
    "section": "1.2 Antecedentes históricos",
    "text": "1.2 Antecedentes históricos\nEl interés por los datos espaciales se remonta a siglos atrás, cuando se comenzaron a representar fenómenos geográficos mediante mapas. Un ejemplo temprano es el trabajo de Halley (1686), quien superpuso en un mapa terrestre las direcciones de los vientos alisios y monzones, con el objetivo de comprender sus causas físicas.\nAunque estas representaciones eran esencialmente descriptivas, los modelos estadísticos espaciales surgieron mucho después. Student (1907) fue uno de los primeros en cuantificar datos espaciales al analizar conteos de partículas por unidad de área, encontrando que el número de células de levadura por cuadrado seguía una distribución de Poisson.\nFisher reconoció explícitamente la existencia de correlación espacial en experimentos agrícolas y de muestreo. En la Estación Experimental de Rothamsted, en las décadas de 1920 y 1930, promovió técnicas como aleatorización, bloqueo y replicación para controlar los efectos de la dependencia espacial. No obstante, estas estrategias sólo son eficaces para escalas espaciales comparables a las dimensiones experimentales; la correlación a otras escalas persiste.\nFairfield Smith (1938) también abordó la dependencia espacial al estudiar cómo el tamaño de las parcelas influye en la varianza del error, reconociendo implícitamente la necesidad de modelar la estructura espacial. Sin embargo, nso fue hasta trabajos como el de Whittle (1954) que comenzaron a desarrollarse modelos formales para describir procesos espaciales.\nHoy en día, el análisis de datos espaciales es fundamental en disciplinas como la geología, la ecología, las ciencias ambientales y la medicina. En muchos de estos contextos, no es viable aplicar métodos experimentales clásicos, lo que impulsa la necesidad de modelos estadísticos que reconozcan explícitamente la dependencia espacial.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "Caps/02-Exploratorio.html",
    "href": "Caps/02-Exploratorio.html",
    "title": "2  Análisis Exploratorio",
    "section": "",
    "text": "2.1 Tipos de datos espaciales\nEn esta primera sección se revisan herramientas básicas para un análisis descriptivo de datos espaciales.\nComo en otros modelos estadísticos, un punto de partida es revisar sobre qué tipo de objeto probabilista se busca hacer inferencia. Es muy útil recordar primero la noción de proceso estocástico tal como se da en (Klenke 2020).\nMuchas de las técnicas para estudiar estos procesos que varían en el tiempo dependen de que \\(I\\) es un conjunto ordenado. Además, para cada \\(\\omega\\in\\Omega\\), el mapeo \\(t\\mapsto Z_t(\\omega)\\) indica lo que hace el proceso a tiempo \\(t\\). A partir de esto, es inmediato extender la definición a un proceso espacial (o espacio-temporal).\nEn este texto se manejarán únicamente procesos espaciales, es decir, consideraremos que \\(t\\) es fijo. A una realización del proceso le denotaremos por \\(z={(z(x))}_{x\\in A}\\). En el contexto del análisis estadístico, se consideran distintos tipos de datos (o realizaciones ed \\(Z\\)).\nA partir de la estructura de esta realización \\(z\\), se pueden considerar distintos tipos de datos espaciales. De manera no exhaustiva, a continuación se presentan algunos de estos tipos de datos.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio</span>"
    ]
  },
  {
    "objectID": "Caps/02-Exploratorio.html#tipos-de-datos-espaciales",
    "href": "Caps/02-Exploratorio.html#tipos-de-datos-espaciales",
    "title": "2  Análisis Exploratorio",
    "section": "",
    "text": "2.1.1 Datos de área o en retícula\nSi \\(A\\) es una unión numerable de unidades de área, decimos que la muestra \\(z\\) son datos de área o de retícula. Este tipo de estructuras es común en imágenes satelitales, en el análisis de suelos y algunos estudios experimentales en agricultura. También se encuentran las imágenes médicas y la teledetección. Los datos pueden representar toda la población (imagen completa) o una submuestra.\nEl énfasis en hablar de “unidades de área” tiene más sentido al considerar que un mapa con división territorial cumple con estas propiedades. Siguiendo a (Moraga 2023), a continuación se presentan un mapa del número de muertes repentinas de niños en cada condado de Carolina del Norte, EE.UU., en el año de 1974, reportado originalmente en (Pebesma 2022a).\n\n\nCódigo\nlibrary(sf)\nlibrary(mapview)\nd &lt;- st_read(system.file(\"shape/nc.shp\", package = \"sf\"), quiet = TRUE)\nmapview(d, zcol = \"SID74\")\n\n\n\n\n\n\nEn el anterior ejemplo, cada región del espacio es un condado. A continuación se presenta un mapa sobre la elevación de Luxemburgo, en el cual todas las regiones del mismo tamaño y están distribuidas de manera uniforme en el espacio, también tomado de (Moraga 2023).\n\n\nCódigo\nlibrary(terra)\nd &lt;- rast(system.file(\"ex/elev.tif\", package = \"terra\"))\nplot(d)\n\n\n\n\n\n\n\n\n\n\n\n2.1.2 Datos geoestadísticos\nEstos datos se miden de manera continua en el espacio, por lo que se consideran formas posiblemente irregulares. Una complicación de este tipo de datos es que normalmente sólo se dispone de observaciones en algunos puntos del espacio. Estos datos aparecen típicamente en cuestiones mineras, de ciencias del suelo, hidrológicas, meteorológicas u otros temas geológicos.\nComo ejemplo, se muestra la distribución de plomo superficial en mg por kg de tierra, en un terreno muestreado tras una inundación del río Meuse en Países Bajos. Estos datos son de la librería sp de R.\n\n\nCódigo\nlibrary(sp)\nlibrary(sf)\nlibrary(mapview)\n\ndata(meuse)\nmeuse &lt;- st_as_sf(meuse, coords = c(\"x\", \"y\"), crs = 28992)\nmapview(meuse, zcol = \"lead\",  map.types = \"CartoDB.Voyager\")\n\n\n\n\n\n\nEste tipo de datos se obtienen con mediciones de infiltrómetros revisando la capacidad del suelo para absorber agua.\nEn este tipo de datos, las ubicaciones de muestreo son generalmente irregulares y se tiene como objetivo predecir la variable en ubicaciones no observadas. Esto se hace bajo la hipótesis de continuidad espacial del campo y que la correlación entre observaciones depende de la distancia.\n\n\n2.1.3 Patrones puntuales\nEn este tipo de datos se centra el estudio en capítulos posteriores y en el caso de estudio. Aquí el dominio \\(A\\) se toma como aleatorio y las variables en \\(Z\\) sólo valen 1 o 0, indicando la ocurrencia o no de un evento. Estos patrones surgen de ubicar eventos, tales como aparición de enfermedades, nacimiento de especies, ubicación de viviendas, entre otros.\nComo ejemplo, se presentan los datos de pinos de hoja larga en Georgia, los cuales se pueden encontrar en la biblioteca spatstat de R. Lo que se mide es la presencia de pinos con longitud de hoja mayor a cierto parámetro. El objetivo del estudio era ver si los árboles con estas características están distribuidos uniformemente al azar en la región o si se tiene algún tipo de agrupamiento o regularidad.\n\n\nCódigo\nlibrary(spatstat)\n\ndata(longleaf)\n\nplot(longleaf, use.marks = FALSE, pch = 20, cex = 0.5, main = \"Pinos de hoja larga en Georgia\")\n\n\n\n\n\n\n\n\n\nSe pueden considerar versiones más generales de este tipo de datos, como los datos puntuales marcados. En los datos puntuales marcados se considera que las observaciones pueden ser de distintos tipos dada alguna característica cualitativa.\nLa característica principal de este tipo de datos es que el interés está centrado en la posición de los eventos, ya que no hay variables medidas continuamente en el espacio.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio</span>"
    ]
  },
  {
    "objectID": "Caps/02-Exploratorio.html#herramientas-descriptivas",
    "href": "Caps/02-Exploratorio.html#herramientas-descriptivas",
    "title": "2  Análisis Exploratorio",
    "section": "2.2 Herramientas descriptivas",
    "text": "2.2 Herramientas descriptivas\nPara el caso de datos puntuales, es posible dar una noción de media y de varianza para datos en cualquier dimensión \\(d\\geq 1\\).\n\nDefinición 2.4 (Centroide y dispersión de una muestra) Sean \\(x=\\{x_1,\\ldots,x_n\\}\\subseteq\\mathbb{R}^d\\) puntos en el espacio, donde \\(x_i=(x_i^{(1)},\\ldots,x_i^{(d)})\\), con \\(i=1,\\ldots,n\\).\n\nEl centroide espacial de \\(x\\) está dado por\n\n\\[\\overline{x}=\\frac{1}{n}\\sum_{i=1}^nx_i=\\left(\\frac{1}{n}\\sum_{i=1}^nx_i^{(1)},\\ldots,\\frac{1}{n}\\sum_{i=1}^nx_i^{(d)}\\right).\\]\n\nLa dispersión espacial de \\(x\\) está dada por\n\n\\[\\sigma^2=\\frac{1}{n}\\sum_{i=1}^n{\\|x_i-\\overline{x}\\|}^2,\\]\ndonde \\(\\|\\cdot\\|\\) es la norma euclidiana de \\(\\mathbb{R}^d\\).\n\nLa matriz de dispersión espacial de \\(x\\) está dada por\n\n\\[\\Sigma=\\frac{1}{n}\\sum_{i=1}^n(x_i-\\overline{x}){(x_i-\\overline{x})}^T.\\]\n\nPara patrones de puntos más complejos como en el caso de los datos del río Meuse, se puede hablar de centros de masa ya que se considera que cada punto en el mapa tiene un valor asignado.\n\nDefinición 2.5 (Centroide de masa y dispersión de masa de una muestra) Sean \\(x=\\{x_1,\\ldots,x_n\\}\\subseteq\\mathbb{R}^d\\) puntos en el espacio, donde \\(x_i=(x_i^{(1)},\\ldots,x_i^{(d)})\\) con pesos \\(w_i\\neq 0\\), con \\(i=1,\\ldots,n\\).\n\nEl centroide de masa espacial de \\(x\\) está dado por\n\n\\[\\overline{x}_w=\\frac{1}{W}\\sum_{i=1}^nw_ix_i=\\left(\\frac{1}{n}\\sum_{i=1}^nw_ix_i^{(1)},\\ldots,\\frac{1}{n}\\sum_{i=1}^nw_ix_i^{(d)}\\right)\\quad\\text{donde}\\quad W=\\sum_{i=1}^nw_i.\\]\n\nLa dispersión de masa espacial de \\(x\\) está dada por\n\n\\[\\sigma^2_w=\\frac{1}{W}\\sum_{i=1}^nw_i{\\|x_i-\\overline{x}_w\\|}^2,\\]\ndonde \\(\\|\\cdot\\|\\) es la norma euclidiana de \\(\\mathbb{R}^d\\).\n\nLa matriz de dispersión de masa espacial de \\(x\\) está dada por\n\n\\[\\Sigma=\\frac{1}{W}\\sum_{i=1}^nw_i(x_i-\\overline{x}_w){(x_i-\\overline{x}_w)}^T.\\]\n\nEste tipo de estadísticos capturan no sólo dónde están los eventos, sino dónde se concentra el fenómeno. Siguiendo con los datos de Meuse, los centroides y las dispersiones para los datos de plomo se muestran a continuación.\n\n\nCódigo\nlibrary(sp)\nlibrary(sf)\nlibrary(mapview)\nlibrary(dplyr)\n\ndata(meuse)\nmeuse_sf &lt;- st_as_sf(meuse, coords = c(\"x\", \"y\"), crs = 28992)\ncoords &lt;- st_coordinates(meuse_sf)\nweights &lt;- meuse_sf$lead\n\ncentroide &lt;- colMeans(coords)\nW &lt;- sum(weights)\ncentro_masa &lt;- colSums(weights * coords) / W\n\ndisp_espacial &lt;- mean(rowSums((coords - centroide)^2))\ndisp_masa &lt;- sum(weights * rowSums((coords - centro_masa)^2)) / W\n\np_centroide &lt;- st_sf(tipo = \"Centroide\", geometry = st_sfc(st_point(centroide)), crs = 28992)\np_masa &lt;- st_sf(tipo = \"Centro de masa\", geometry = st_sfc(st_point(centro_masa)), crs = 28992)\n\nmapview(meuse_sf, zcol = \"lead\", map.types = \"CartoDB.Voyager\") +\n  mapview(p_centroide, color = \"blue\", col.region = \"blue\", label = \"Centroide\") +\n  mapview(p_masa, color = \"red\", col.region = \"red\", label = \"Centro de masa\")\n\n\n\n\n\n\nCódigo\nlibrary(knitr)\ndata.frame(\n  Medida = c(\"Centroide X\", \"Centroide Y\", \"Centro de masa X\", \"Centro de masa Y\",\n             \"Dispersión espacial\", \"Dispersión ponderada\"),\n  Valor = c(round(centroide[1], 1), round(centroide[2], 1),\n            round(centro_masa[1], 1), round(centro_masa[2], 1),\n            round(disp_espacial, 2), round(disp_masa, 2))\n) %&gt;%\n  kable(caption = \"Resumen de características espaciales de los datos.\")\n\n\n\nResumen de características espaciales de los datos.\n\n\nMedida\nValor\n\n\n\n\nCentroide X\n1.800046e+05\n\n\nCentroide Y\n3.316349e+05\n\n\nCentro de masa X\n1.799196e+05\n\n\nCentro de masa Y\n3.316878e+05\n\n\nDispersión espacial\n2.284400e+10\n\n\nDispersión ponderada\n2.278173e+10\n\n\n\n\n\nSi se tratara con datos a una escala muy grande, digamos datos en los que los datos están distribuidos a lo largo de todo el continente americano, es necesario tomar consideraciones topológicas de curvatura y restricciones a estar sobre el mapa. Esto ya que con las definiciones dadas podría ser que los puntos ni siquiera estén sobre el mapa (basta tomar el polo norte y el polo sur para tener un ejemplo de esto).\n\n2.2.1 Visualización de datos\nAunque las estadísticas anteriores sí capturan información relevante para los datos, se pierde mucha información sobre la estructura espacial del fenómeno. Existen diversos recursos gráficos para poder discutir sobre datos espaciales, algunos de éstos se presentan en las siguientes secciones. Más sobre éstos y otros tipos de representaciones gráficas de datos espaciales se pueden ver en (Bivand, Pebesma, y Gomez-Rubio 2013) y en (Baddeley, Rubak, y Turner 2015).\n\nMapa de símbolos proporcionales\nUn mapa de símbolos proporcionales es un tipo de gráfico que representa la magnitud de una variable en ubicaciones puntuales mediante símbolos cuyo tamaño es proporcional al valor observado.\nOtra manera de visualizar los datos de pinos de hoja larga de la sección Sección 2.1.3 es la siguiente\n\n\nCódigo\nlibrary(spatstat)\n\ndata(longleaf)\n\nplot(longleaf, maxsize=15, main = \"Pinos de hoja larga en Georgia con diámetro\")\n\n\n\n\n\n\n\n\n\nLos diámetros de los árboles no están a escala: la leyenda muestra los diámetros en centímetros.\n\n\nMapas de color\nMuestra la densidad o intensidad de eventos sobre una superficie. Siguiendo el ejemplo de los pinos de hoja larga\n\n\nCódigo\nlibrary(spatstat)\n\ndata(longleaf)\n\nA &lt;- colourmap(heat.colors(128), range=range(marks(longleaf)))\n\nplot(longleaf,  pch=21, bg=A, cex=1, main = \"Pinos de hoja larga en Georgia con diámetro\")\n\n\n\n\n\n\n\n\n\n\n\nMapa de contornos\nUn mapa de contorno (o gráfico de curvas de nivel) es una representación visual de una superficie tridimensional (como una montaña, un terreno o cualquier variable continua) en un plano bidimensional. Conecta con curvas de nivel los puntos que tienen el mismo valor en una magnitud medida (altitud, temperatura, presión, etc.).\nA continuación se presenta un mapa de contorno del volcán Monte Edén de Auckland.\n\n\nCódigo\ndata(volcano)\ncontour(volcano, \n        main = \"Altitudes de Maunga Whau\",,\n        col = \"blue\", \n        lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n2.2.2 El variograma\nUna herramienta de gran utilidad en el análisis de este tipo de procesos con estructura adicional es el variograma. La idea de este estadístico es estudiar empíricamente la estructura de covarianza del proceso espacial.\nLa idea se toma del contexto de series de tiempo, donde se busca medir la influencia de tiempos anteriores en el instante de estudio. Algunas de las definiciones de interés en el contexto de las series de tiempo se presentan a continuación, como se enuncian en (Rincón y De Jesús 2025).\n\nDefinición 2.6 (Estructura de covarianza y estacionareidad) Sea \\(X={(X_t)}_{t\\in\\mathbb{Z}}\\) una serie de tiempo.\n\nSi \\(X\\) tiene media finita, la función media es \\(t\\mapsto\\mu(t)=\\mathbb{E}[X_t]\\) para cada \\(t\\in\\mathbb{Z}\\).\nSi \\(X\\) tiene segundo momento finito, la función varianza es \\(t\\mapsto\\text{Var}(X_t)\\) para cada \\(t\\in\\mathbb{Z}\\).\nSi \\(X\\) tiene segundo momento, la función de autocovarianza es \\((s,t)\\mapsto\\gamma(s,t)=\\text{Cov}[X_s,X_t]\\).\n\\(X\\) es estrictamente estacionaria si para cualesquiera \\(k\\geq 1\\) y \\(\\tau\\in\\mathbb{Z}\\) se cumple que \\((X_1,\\ldots,X_k)\\overset{d}{=}(X_{1+\\tau},\\ldots,X_{k+\\tau})\\).\n\\(X\\) es débilmente estacionaria si satisface que \\(\\mu(t)=\\mu\\) y \\(\\gamma(t,t+\\tau)=\\gamma(\\tau)\\).\nSi \\(X\\) es estacionaria, la función de autocorrelación es \\(\\displaystyle\\rho(\\tau)=\\frac{\\gamma(\\tau)}{\\gamma(0)}\\).\n\n\nLa propiedad de ser estacionaria quiere decir que la estructura de correlación depende únicamente del tamaño del periodo que se revise y no de los instantes específicos en los que se vea el proceso, tal tamaño del periodo es la separación entre el inicio y el final del periodo. En el contexto de los datos espaciales, las nociones de estacionaridad se definen en términos de la distancia entre dos puntos.\n\nDefinición 2.7 (Proceso espacial estacionario) Sea \\(Z={(Z(s))}_{s\\in A\\subseteq\\mathbb{R}^d}\\) un proceso espacial. Decimos que \\(Z\\) es un proceso intrínsecamente estacionario si satisface que para todos \\(s,h\\in\\mathbb{R}^d\\)\n\\[\\mathbb{E}[Z(s+h)-Z(s)]=0\\quad\\text{y}\\quad\\text{Var}(Z(s+h)-Z(t))=2\\gamma(h),\\]\ndonde \\(\\gamma\\) es el variograma de \\(Z\\). \\(\\gamma\\) satisface que, para todos \\(s_1,\\ldots,s_k\\in A\\) y \\(\\alpha_1,\\ldots,\\alpha_n\\in\\mathbb{C}\\) tales que \\(\\alpha_1+\\cdots+\\alpha_n=0\\)\n\\[\\sum_{i=1}^k\\sum_{k=1}^k\\alpha_i\\overline{\\alpha}_j2\\gamma(s_i-s_j)\\leq 0.\\]\n\nEn pocas palabras, el variograma es una medida de la tendencia a ser diferentes de los valores según su distancia. Algunos comentarios que se pueden hacer a partir del variograma son las siguientes.\n\nCuando \\(h\\) es pequeño, se espera que \\(\\gamma(h)\\) sea pequeño, es decir, que las observaciones sean similares.\nCuando \\(h\\) crece, usualmente \\(\\gamma(h)\\) suele aumentar.\nA partir de cierto \\(h_0\\), usualmente para \\(h\\geq h_0\\) se tiene que \\(\\gamma(h)\\approx\\text{cte.}\\). A este \\(h_0\\) se le llama rango.\nHay veces que \\(\\gamma(h)\\to\\gamma_0&gt;0\\) cuando \\(h\\to 0\\). A este valor \\(\\gamma_0\\) se le llama efecto nugget (por las pepitas de oro encontradas en ríos, o en inglés, los ‘’gold nuggets’’).\n\nA continuación se presenta un estimador clásico del variograma, el cual fue propuesto por Mathéron en 1962.\n\nDefinición 2.8 (Variograma muestral) El variograma muestral del proceso espacial \\(Z\\) es\n\\[2\\widehat{\\gamma}(h)=\\frac{1}{|N(h)|}\\sum_{N(h)}{(Z(s_i)-Z(s_j))}^2,\\]\ndonde \\(N(h)=\\{(i,j)\\ :\\ s_i-s_j=h\\}\\) y \\(|N(h)|\\) es el número de elementos distintos en \\(N(h)\\).\n\nUna de las principales desventajas de este estimador es su sensibilidad a valores grandes de \\(Z\\). En (Cressie 2015) se puede revisar una variedad de ejemplos del uso de variograma. Aquí se da seguimiento al ejemplo del río Meuse de la sección Sección 2.1.2, presentando el variograma de los datos del plomo.\n\n\nCódigo\nlibrary(sp)\nlibrary(sf)\nlibrary(gstat)\n\ndata(meuse)\nmeuse_sp &lt;- meuse\ncoordinates(meuse_sp) &lt;- ~x + y\n\nvg_lead &lt;- variogram(log(lead) ~ 1, data = meuse_sp, cloud = FALSE)\n\nplot(vg_lead$dist, vg_lead$gamma, \n     xlab = \"Distancia (m)\", \n     ylab = \"Semivarianza\",\n     pch = 19, col = \"red\",\n     main = \"Variograma muestral de Plomo (log-escala)\",\n     cex.main = 1.2, cex.lab = 1.1)\n\ntext(vg_lead$dist, vg_lead$gamma, \n     labels = vg_lead$np, \n     pos = 3, cex = 0.8, col = \"blue\")\n\n\n\n\n\n\n\n\n\nLos números sobre cada punto en el mapa representan el número de pares de observaciones de \\(|N(h)|\\) usados para la semivarianza. La utilidad de estos números radica en que\n\nDan un criterio para medir la confiabilidad de cada punto: si un punto se basa en pocos pares, la estimación es menos confiable.\nAyudan a evaluar la dispersión de las distancias: muchos pares en una banda significa que esa distancia es común en el diseño del experimento.\n\n\n\n\n\nBaddeley, Adrian, Ege Rubak, y Thomas Turner. 2015. Spatial Point Patterns: Methodology and Applications with R. First. Chapman; Hall/CRC.\n\n\nBivand, Roger, Edzer Pebesma, y Virgilio Gomez-Rubio. 2013. Applied Spatial Data Analysis with R. Second. Springer.\n\n\nCressie, Noel. 2015. Statistics for Spatial Data. Revised. Wiley.\n\n\nKlenke, Achim. 2020. Probability Theory: A Comprehensive Course. Third. Wiley.\n\n\nMoraga, Paula. 2023. Spatial Statistics for Data Science: Theory and Practice with R. First. Chapman & Hall/CRC.\n\n\nPebesma, Edzer. 2022a. Simple Features for R. R-project.\n\n\nRincón, Luis, y Verónica De Jesús. 2025. Una introducción a las series de tiempo. First. La prensa de Ciencias.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio</span>"
    ]
  },
  {
    "objectID": "Caps/02-Visualizacion.html",
    "href": "Caps/02-Visualizacion.html",
    "title": "3  Visualización de datos espaciales",
    "section": "",
    "text": "3.1 Clase 1\nComo el título lo indica, en este capítulo se presentan algunas herramientas de visualización de datos espaciales. El material base es el mini curso de Análisis de datos espaciales con R impartido por Noé Osorio García. Se asume que el lector está familiarizado con las funciones básicas de R.\nLas recomendaciones técnicas para seguir el material son:\nEl temario propuesto es\nEl objetivo de la primera sesión fue dar nociones básicas de R: operaciones de aritmética, tipos de datos y estructuras básicas de datos; y una aproximación en visualización con un mapa dinámico elaborado con leaflet.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Visualización de datos espaciales</span>"
    ]
  },
  {
    "objectID": "Caps/02-Visualizacion.html#clase-1",
    "href": "Caps/02-Visualizacion.html#clase-1",
    "title": "3  Visualización de datos espaciales",
    "section": "",
    "text": "3.1.1 Introducción a R\n\n\n3.1.2 RStudio\n\n\n3.1.3 Objetos y tipos de datos en R\n\nVector\n\n\nMatriz\n\n\nData Frame\n\n\n\n3.1.4 Funciones de aritmética\n\n\n3.1.5 Condiciones lógicas\n\n\n3.1.6 Asignación\n\n\n3.1.7 Operaciones con vectores\n\n\n3.1.8 Cargado de datasets\nNormalmente se trabaja con información ya existente de sitios web, libros de Excel, información de encuestas, bases gubernamentales, entre otros. Con R el crear bases de datos no es el objetivo primario, sino analizar la información existente.\n\n\n\nFormato\nFunción base\nFunción tidyverse\nPaquete requerido\n\n\n\n\nCSV\nread.csv()\nread_csv()\nreadr\n\n\nExcel\n-\nread_excel()\nreadxl\n\n\nSPSS\nread.spss()\n-\nforeign\n\n\nStata\nread.dta()\n-\nforeign\n\n\nSAS\n-\nread_sas()\nhaven\n\n\nDBF\nread.dbf()\n-\nforeign\n\n\nShapefile\n-\nst_read()\nsf\n\n\ngpkg\n-\nst_read()\nsf\n\n\nkml\n-\nst_read()\nsf\n\n\nraster,tiff\n-\nrast()\nterra\n\n\nJSON\n-\nfromJSON()\njsonlite\n\n\nXML\n-\nread_xml()\nxml2\n\n\n\nLas dos librerías fundamentales para facilitar el procesamiento y la visualización de datos que se consideran son\n\ntidyverse: conjunto de paquetes para análisis de datos\n\ndplyr: manipulación y transformación de datos\nggplot2: visualización a través de la gramática de grafos\ntidyr: ordenar y limpiar datos\nreadr: importación de archivos CSV/TSV\npurrr: programación funcional\nstringr: manipulación de cadenas de texto\nforcats: manejo de factores (variables categóricas)\n\nleaflet: librería para crear mapas interactivos. Es la implementación en R de la biblioteca leaflet de JavaScript\n\nleaflet.extras\nleaflet.extras2\nleafsync\n\n\n\n\n3.1.9 Ejemplo trabajado\nEn esta primera sesión se trabaja con datos de la Fiscalía General de Justicia de la Ciudad de México. Más específicamente, con datos de víctimas en carpetas de investigación (2024). La función read_csv`` puede recibir una URL o una ruta de archivo local. Con la funciónglimpse` es posible saber qué tipo de dato tiene cada columna y algunos valores de los que contiene.\n\n\nCódigo\nlibrary(tidyverse)\n\nbase = read_csv(\"https://archivo.datos.cdmx.gob.mx/FGJ/victimas/victimasFGJ_2024.csv\")\n\nglimpse(base)\n\n\nRows: 146,616\nColumns: 22\n$ anio_inicio       &lt;dbl&gt; 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024…\n$ mes_inicio        &lt;chr&gt; \"Enero\", \"Enero\", \"Enero\", \"Enero\", \"Enero\", \"Enero\"…\n$ fecha_inicio      &lt;date&gt; 2024-01-01, 2024-01-01, 2024-01-01, 2024-01-01, 202…\n$ hora_inicio       &lt;time&gt; 00:20:00, 01:12:00, 01:14:00, 02:29:00, 03:26:00, 0…\n$ anio_hecho        &lt;dbl&gt; 2024, 2023, 2023, 2023, 2024, 2024, 2024, 2024, 2023…\n$ mes_hecho         &lt;chr&gt; \"Enero\", \"Diciembre\", \"Diciembre\", \"Diciembre\", \"Ene…\n$ fecha_hecho       &lt;date&gt; 2024-01-01, 2023-12-28, 2023-12-31, 2023-12-31, 202…\n$ hora_hecho        &lt;time&gt; 00:20:00, 21:45:00, 23:00:00, 23:50:00, 02:10:00, 0…\n$ delito            &lt;chr&gt; \"PERDIDA DE LA VIDA POR OTRAS CAUSAS\", \"HOMICIDIO CU…\n$ categoria_delito  &lt;chr&gt; \"HECHO NO DELICTIVO\", \"DELITO DE BAJO IMPACTO\", \"HEC…\n$ sexo              &lt;chr&gt; \"Masculino\", \"Femenino\", \"Masculino\", \"Femenino\", \"F…\n$ edad              &lt;dbl&gt; 50, 42, 73, 23, 26, 66, 65, 38, 39, 35, 60, 80, NA, …\n$ tipo_persona      &lt;chr&gt; \"FISICA\", \"FISICA\", \"FISICA\", \"FISICA\", \"FISICA\", \"F…\n$ calidad_juridica  &lt;chr&gt; \"CADAVER\", \"CADAVER\", \"CADAVER\", \"DENUNCIANTE Y VICT…\n$ competencia       &lt;chr&gt; \"HECHO NO DELICTIVO\", \"FUERO COMUN\", \"HECHO NO DELIC…\n$ colonia_hecho     &lt;chr&gt; \"SAN MIGUEL TOPILEJO\", \"AERONÁUTICA MILITAR\", \"OJITO…\n$ colonia_catalogo  &lt;chr&gt; \"Pueblo San Miguel Topilejo\", \"Aeronautica Militar\",…\n$ alcaldia_hecho    &lt;chr&gt; \"TLALPAN\", \"VENUSTIANO CARRANZA\", \"IZTAPALAPA\", \"CUA…\n$ alcaldia_catalogo &lt;chr&gt; \"Tlalpan\", \"Venustiano Carranza\", \"Iztapalapa\", \"Cua…\n$ municipio_hecho   &lt;chr&gt; \"CDMX\", \"CDMX\", \"CDMX\", \"CDMX\", \"CDMX\", \"CDMX\", \"CDM…\n$ latitud           &lt;dbl&gt; 19.19861, 19.42063, 19.34726, 19.43405, 19.31416, 19…\n$ longitud          &lt;dbl&gt; -99.14006, -99.11540, -99.02453, -99.13484, -99.2639…\n\n\n\nEstructura de la base\nPara explorar datos, un buen primer paso es realizar conteos para entender la distribución de las variables. La manera más simple de hacerla es con el operador $ y la función table() para obtener frecuencias. Para el caso de la columna categoria_delito, por ejemplo:\n\n\nCódigo\ntable(base$categoria_delito)\n\n\n\n                                 DELITO DE BAJO IMPACTO \n                                                 128184 \n                                     HECHO NO DELICTIVO \n                                                   3679 \n                                       HOMICIDIO DOLOSO \n                                                    773 \n          LESIONES DOLOSAS POR DISPARO DE ARMA DE FUEGO \n                                                    555 \n                   ROBO A CASA HABITACIÓN CON VIOLENCIA \n                                                    151 \nROBO A CUENTAHABIENTE SALIENDO DEL CAJERO CON VIOLENCIA \n                                                     86 \n                           ROBO A NEGOCIO CON VIOLENCIA \n                                                   1103 \nROBO A PASAJERO A BORDO DE MICROBUS CON Y SIN VIOLENCIA \n                                                    242 \n          ROBO A PASAJERO A BORDO DE TAXI CON VIOLENCIA \n                                                     90 \n  ROBO A PASAJERO A BORDO DEL METRO CON Y SIN VIOLENCIA \n                                                    719 \n                  ROBO A REPARTIDOR CON Y SIN VIOLENCIA \n                                                    470 \n   ROBO A TRANSEUNTE EN VÍA PÚBLICA CON Y SIN VIOLENCIA \n                                                   4661 \n               ROBO A TRANSPORTISTA CON Y SIN VIOLENCIA \n                                                     32 \n                   ROBO DE VEHÍCULO CON Y SIN VIOLENCIA \n                                                   4241 \n                                              SECUESTRO \n                                                      8 \n                                              VIOLACIÓN \n                                                   1622 \n\n\nEs importante notar que esta función omite los valores NA dentro de la columna, pero se pueden agregar con un ìfany\n\n\nCódigo\ntable(base$sexo)\n\n\n\n        Femenino        Masculino No se especifica \n           62368            60989                1 \n\n\n\n\nCódigo\ntable(base$sexo,useNA=\"ifany\")\n\n\n\n        Femenino        Masculino No se especifica             &lt;NA&gt; \n           62368            60989                1            23258 \n\n\n\n\nFiltrado de la base de datos\nEs común que sólo nos interese una parte del data set. Para filtrar es necesario poner el término clave tal y como está escrito, esto en la función filter(). Se pueden utilizar las funciones table() y unique() para poder ver cómo están escritos los datos.\n\n\nCódigo\nunique(base$categoria_delito)\n\n\n [1] \"HECHO NO DELICTIVO\"                                     \n [2] \"DELITO DE BAJO IMPACTO\"                                 \n [3] \"ROBO A TRANSEUNTE EN VÍA PÚBLICA CON Y SIN VIOLENCIA\"   \n [4] \"HOMICIDIO DOLOSO\"                                       \n [5] \"ROBO DE VEHÍCULO CON Y SIN VIOLENCIA\"                   \n [6] \"ROBO A NEGOCIO CON VIOLENCIA\"                           \n [7] \"ROBO A PASAJERO A BORDO DEL METRO CON Y SIN VIOLENCIA\"  \n [8] \"ROBO A PASAJERO A BORDO DE MICROBUS CON Y SIN VIOLENCIA\"\n [9] \"ROBO A PASAJERO A BORDO DE TAXI CON VIOLENCIA\"          \n[10] \"VIOLACIÓN\"                                              \n[11] \"LESIONES DOLOSAS POR DISPARO DE ARMA DE FUEGO\"          \n[12] \"ROBO A REPARTIDOR CON Y SIN VIOLENCIA\"                  \n[13] \"ROBO A CASA HABITACIÓN CON VIOLENCIA\"                   \n[14] \"ROBO A CUENTAHABIENTE SALIENDO DEL CAJERO CON VIOLENCIA\"\n[15] \"ROBO A TRANSPORTISTA CON Y SIN VIOLENCIA\"               \n[16] \"SECUESTRO\"                                              \n\n\nSe crearán cuatro bases con distintas categorías de delito y se asignarán a nuevos objetos: delito_1, delito_2, delito_3 y delito_4.\n\n\nCódigo\ndelito_1 = base %&gt;%\n  filter(categoria_delito==\"ROBO A NEGOCIO CON VIOLENCIA\")\ndelito_2 = base %&gt;%\n  filter(categoria_delito==\"HOMICIDIO DOLOSO\")\ndelito_3 = base %&gt;%\n  filter(categoria_delito==\"ROBO A CASA HABITACIÓN CON VIOLENCIA\")\ndelito_4 = base %&gt;%\n  filter(categoria_delito==\"VIOLACIÓN\")\n\n\n\n\nPrimeros mapas en R\nPara la visualización con la biblioteca leaflet, el proceso es\n\nLlamar a la biblioteca\nUsar la función leaflet\nElegir la geometría\nGuardarlo en un objeto\n\n\nPrimer mapa\nEl primer mapa que hacemos es con base_1 que contiene la categoría de ROBO A NEGOCIO CON VIOLENCIA. Para crear el mapa necesitamos: un data set, la biblioteca leaflet y el tipo de mapa. Los pasos, en este caso, son:\n\nLlamar a leaflet\nAgregar un mapa base. Hay distintos proveedores, dentro del vector providers$ se puede cambiar\nAgregar el objeto que queremos ver. Usualmente se añade con add + el tipo de objeto. Si son polígonos, usamos addPolygons. Si son datos de latitud y longitud (latlong), usamos addCircles, recibe tres parámetros obligatorios: data, lng y lat. Para líneas, usamos addpolylines.\n\n\n\nCódigo\nlibrary(leaflet)\n\nmapa_1 = leaflet() %&gt;%\n  addProviderTiles(provider = providers$CartoDB) %&gt;%\n  addCircles(data=delito_1,\n  lng = ~longitud,\n  lat = ~latitud)\n\nmapa_1\n\n\n\n\n\n\n\n\nSegundo mapa\nEl segundo mapa que hacemos es con base_2. Para este mapa, hacemos que los puntos sean de color rojo con red. En colorbrewer2 se pueden consultar paletas.\n\n\nCódigo\nmapa_2 = leaflet() %&gt;%\n    addProviderTiles(provider = providers$OpenStreetMap) %&gt;%\n    addCircles(data=delito_2,\n    lng = ~longitud,\n    lat = ~latitud,\n    opacity = 1,\n    color = \"red\")\n\nmapa_2\n\n\n\n\n\n\n\n\nTercer mapa\nEl tercer mapa que hacemos es con base_4. En leaflet se pueden agregar geometrías conectando con el operador %&gt;% después de addCircle. En este caso, agregamos addCircleMarkers, el cual agrega el parámetro de clusterOptions.\n\n\nCódigo\nmapa_3 = leaflet() %&gt;%\n    addProviderTiles(provider = providers$Esri.WorldGrayCanvas) %&gt;%\n    addCircles(data=delito_3,\n        lng = ~longitud,\n        lat = ~latitud,\n        opacity = 1,\n        color = \"black\") %&gt;%\n    addCircleMarkers(data=delito_3,\n        lng = ~longitud,\n        lat = ~latitud,\n        color = \"black\",\n        clusterOptions = TRUE)\n\nmapa_3\n\n\n\n\n\n\nResaltamos la diferencia entre CartoDB, OpenStreetMap y Esri.WorldGrayCanvas.\n\n\nCuarto mapa\nEl mensaje de error que dice Aviso: Data contains 175 rows with either missing or invalid lat/lng values and will be ignored. Este mensaje indica que el data set contiene \\(x\\) cantidad de renglones con entradas lat y lng con valor NA. No pasa nada si no los quitamos, pero se pueden filtrar con la condición !is.na(). La función heatmap tiene los mismos tres parámetros obligatorios pero tambié tiene radius, la cual está en metros para cada punto y minOpacity con rango de 0 a 1, donde 1 es sin transparencia.\n\n\nCódigo\nlibrary(leaflet.extras)\n\ndelito_4 = delito_4 %&gt;%\n    filter(!is.na(longitud),\n           !is.na(latitud))\n\nmapa_4 = leaflet() %&gt;%\n    addProviderTiles(provider = providers$CartoDB) %&gt;%\n    addHeatmap(data=delito_4,\n        lng = ~longitud,\n        lat = ~latitud,\n        radius = 15,\n        minOpacity = 1) %&gt;%\n    addCircleMarkers(data=delito_4,\n        lng = ~longitud,\n        lat = ~latitud,\n        clusterOptions = TRUE)\n\nmapa_4\n\n\n\n\n\n\n\n\nVisualización simultánea\nAlgunas veces puede ser útil visualizar un mapa de distintas variables en la misma zona, e igual analizarlos al mismo tiempo. Una opción es sincronizar los mapas. Para esto, utilizamos la biblioteca leafsync.\n\n\nCódigo\nlibrary(leafsync)\n\nleafsync::sync(\n    mapa_1,mapa_2,mapa_3,mapa_4,\n    ncol = 2\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTambién se pueden visualizar simultáneamente sin estar sincronizados, esto con crosstalk (aunque esto prácticamente crea el arreglo de las cuatro figuras).\n\n\nCódigo\nlibrary(crosstalk)\n\ncrosstalk::bscols(\n  mapa_1, mapa_2,\n  mapa_3, mapa_4,\n  widths = c(6, 6)\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPara publicar mapas en la web usando RStudio, hay que tener cuenta de rpubs y de ahí dar un publish.\n\n\n\n\n3.1.10 Referencias\nEn esta clase se utilizaron principalmente el capítulo 5 de (Wickham y Grolemund 2017) y la introducción de (Lovelace, Nowosad, y Muenchow 2025).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Visualización de datos espaciales</span>"
    ]
  },
  {
    "objectID": "Caps/02-Visualizacion.html#clase-2",
    "href": "Caps/02-Visualizacion.html#clase-2",
    "title": "3  Visualización de datos espaciales",
    "section": "3.2 Clase 2",
    "text": "3.2 Clase 2\nEl objetivo de la segunda sesión fue conocer funciones de R para manejar data frames: agregar columnas, filtros y conteos básicos. También se visualizaron datos no sólo en mapas, se presenta una manera de pasar de puntos a polígonos creando un mapa de hexbin. Para visualización dinámica se usa leaflet y para visualización estática se usa ggplot.\nLos datos utilizados en esta clase fueron también de la FGJ, pero en esta ocasión se utilizó el acumulado, para poder utilizar el tiempo como parámetero para comparar alguna evolución.\nEn el sitio de la fiscalía, basta cargar el acumulado utilizando tidyverse como anteriormente.\n\n\nCódigo\nlibrary(tidyverse)\n\nbase = read_csv(\"https://archivo.datos.cdmx.gob.mx/FGJ/victimas/victimasFGJ_acumulado_2024_09.csv\")\n\n\nLas variables y sus tipos, con los que contamos desde ahora, se muestran a continuación.\n\n\nCódigo\nbase %&gt;% glimpse()\n\n\nRows: 1,415,763\nColumns: 22\n$ anio_inicio       &lt;dbl&gt; 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019…\n$ mes_inicio        &lt;chr&gt; \"Enero\", \"Enero\", \"Enero\", \"Enero\", \"Enero\", \"Enero\"…\n$ fecha_inicio      &lt;date&gt; 2019-01-04, 2019-01-04, 2019-01-04, 2019-01-04, 201…\n$ hora_inicio       &lt;time&gt; 12:19:00, 12:20:00, 12:23:00, 12:27:00, 12:35:00, 1…\n$ anio_hecho        &lt;dbl&gt; 2018, 2018, 2018, 2019, 2019, 2018, 2019, 2018, 2018…\n$ mes_hecho         &lt;chr&gt; \"Agosto\", \"Diciembre\", \"Diciembre\", \"Enero\", \"Enero\"…\n$ fecha_hecho       &lt;date&gt; 2018-08-29, 2018-12-15, 2018-12-22, 2019-01-04, 201…\n$ hora_hecho        &lt;time&gt; 12:00:00, 15:00:00, 15:30:00, 06:00:00, 20:00:00, 1…\n$ delito            &lt;chr&gt; \"FRAUDE\", \"PRODUCCIÓN, IMPRESIÓN, ENAJENACIÓN, DISTR…\n$ categoria_delito  &lt;chr&gt; \"DELITO DE BAJO IMPACTO\", \"DELITO DE BAJO IMPACTO\", …\n$ sexo              &lt;chr&gt; \"Masculino\", \"Femenino\", \"Masculino\", \"Masculino\", \"…\n$ edad              &lt;dbl&gt; 62, 38, 42, 35, NA, 42, 55, 83, 45, 27, 58, 31, 61, …\n$ tipo_persona      &lt;chr&gt; \"FISICA\", \"FISICA\", \"FISICA\", \"FISICA\", \"FISICA\", \"F…\n$ calidad_juridica  &lt;chr&gt; \"OFENDIDO\", \"VICTIMA Y DENUNCIANTE\", \"VICTIMA Y DENU…\n$ competencia       &lt;chr&gt; \"FUERO COMUN\", \"FUERO COMUN\", \"FUERO COMUN\", \"FUERO …\n$ colonia_hecho     &lt;chr&gt; \"GUADALUPE INN\", \"VICTORIA DE LAS DEMOCRACIAS\", \"COP…\n$ colonia_catalogo  &lt;chr&gt; \"Guadalupe Inn\", \"Victoria De Las Democracias\", \"Cop…\n$ alcaldia_hecho    &lt;chr&gt; \"ALVARO OBREGON\", \"AZCAPOTZALCO\", \"COYOACAN\", \"IZTAC…\n$ alcaldia_catalogo &lt;chr&gt; \"Álvaro Obregón\", \"Azcapotzalco\", \"Coyoacán\", \"Iztac…\n$ municipio_hecho   &lt;chr&gt; \"CDMX\", \"CDMX\", \"CDMX\", \"CDMX\", \"CDMX\", \"CDMX\", \"CDM…\n$ latitud           &lt;dbl&gt; 19.36125, 19.47181, 19.33797, 19.40327, 19.35480, 19…\n$ longitud          &lt;dbl&gt; -99.18314, -99.16458, -99.18611, -99.05983, -99.0632…\n\n\n\n3.2.1 Conteos y su visualización\nAl hacer un análisis exploratorio, un buen primer paso para entender la distribución de variables es haciendo conteos de éstas. Con la función count() se puede hacer esto fácilmente, e igualmente se puede agregar el parámetro sort para ordenar de mayor a menor.\n\n\nCódigo\nbase %&gt;% count(sexo)\n\n\n# A tibble: 4 × 2\n  sexo                  n\n  &lt;chr&gt;             &lt;int&gt;\n1 Femenino         563275\n2 Masculino        604782\n3 No se especifica      2\n4 &lt;NA&gt;             247704\n\n\n\n\nCódigo\nbase %&gt;% count(categoria_delito,sort=TRUE)\n\n\n# A tibble: 17 × 2\n   categoria_delito                                              n\n   &lt;chr&gt;                                                     &lt;int&gt;\n 1 DELITO DE BAJO IMPACTO                                  1182776\n 2 ROBO A TRANSEUNTE EN VÍA PÚBLICA CON Y SIN VIOLENCIA      66473\n 3 ROBO DE VEHÍCULO CON Y SIN VIOLENCIA                      51149\n 4 ROBO A NEGOCIO CON VIOLENCIA                              27460\n 5 HECHO NO DELICTIVO                                        26046\n 6 VIOLACIÓN                                                 13352\n 7 ROBO A REPARTIDOR CON Y SIN VIOLENCIA                     12835\n 8 ROBO A PASAJERO A BORDO DEL METRO CON Y SIN VIOLENCIA      8851\n 9 HOMICIDIO DOLOSO                                           8152\n10 LESIONES DOLOSAS POR DISPARO DE ARMA DE FUEGO              6665\n11 ROBO A PASAJERO A BORDO DE MICROBUS CON Y SIN VIOLENCIA    4777\n12 ROBO A CASA HABITACIÓN CON VIOLENCIA                       2872\n13 ROBO A CUENTAHABIENTE SALIENDO DEL CAJERO CON VIOLENCIA    2050\n14 ROBO A PASAJERO A BORDO DE TAXI CON VIOLENCIA              1517\n15 ROBO A TRANSPORTISTA CON Y SIN VIOLENCIA                    572\n16 SECUESTRO                                                   202\n17 &lt;NA&gt;                                                         14\n\n\n\n\nCódigo\nbase %&gt;% count(categoria_delito,sexo)\n\n\n# A tibble: 50 × 3\n   categoria_delito       sexo                  n\n   &lt;chr&gt;                  &lt;chr&gt;             &lt;int&gt;\n 1 DELITO DE BAJO IMPACTO Femenino         501803\n 2 DELITO DE BAJO IMPACTO Masculino        477220\n 3 DELITO DE BAJO IMPACTO No se especifica      2\n 4 DELITO DE BAJO IMPACTO &lt;NA&gt;             203751\n 5 HECHO NO DELICTIVO     Femenino           6355\n 6 HECHO NO DELICTIVO     Masculino         17819\n 7 HECHO NO DELICTIVO     &lt;NA&gt;               1872\n 8 HOMICIDIO DOLOSO       Femenino           1227\n 9 HOMICIDIO DOLOSO       Masculino          6893\n10 HOMICIDIO DOLOSO       &lt;NA&gt;                 32\n# ℹ 40 more rows\n\n\nNotemos que se tienen registros desde el supuesto año 222 o 1917, podemos filtrar esto en los conteos.\n\n\nCódigo\nbase %&gt;%\n    filter(anio_hecho&gt;2018) %&gt;%\n    count(anio_hecho)\n\n\n# A tibble: 6 × 2\n  anio_hecho      n\n       &lt;dbl&gt;  &lt;int&gt;\n1       2019 262156\n2       2020 216317\n3       2021 238715\n4       2022 249729\n5       2023 248339\n6       2024 167901\n\n\nEsto ya se puede graficar. Una gráfica sencilla en ggplot se haría con los siguientes pasos\n\nFiltrar y contar los datos (ya se hizo)\nIniciar el gráfico con ggplot()\nIndicar con geom_col() que se quiere una gráfica de columnas\n\n\n\nCódigo\nbase %&gt;%\n    filter(anio_hecho&gt;2018) %&gt;%\n    count(anio_hecho) %&gt;%\n    ggplot(aes(x=anio_hecho,y=n))+\n    geom_col()\n\n\n\n\n\n\n\n\n\nCuando se realiza el conteo con más de una variable, se pueden empezar a empalmar (somehow) las columnas. Esto se puede resolver con position o con facetas. A continuación se muestran dos gráficas de lo mismo: una en la que se empalman los distintos tipos de delito (que es exactamente igual a la anterior…) y otra donde sí se diferencían.\n\n\nCódigo\nbase %&gt;%\n    filter(anio_hecho&gt;2018) %&gt;%\n    count(anio_hecho,categoria_delito)\n\n\n# A tibble: 96 × 3\n   anio_hecho categoria_delito                                             n\n        &lt;dbl&gt; &lt;chr&gt;                                                    &lt;int&gt;\n 1       2019 DELITO DE BAJO IMPACTO                                  199500\n 2       2019 HECHO NO DELICTIVO                                        3647\n 3       2019 HOMICIDIO DOLOSO                                          1899\n 4       2019 LESIONES DOLOSAS POR DISPARO DE ARMA DE FUEGO             1781\n 5       2019 ROBO A CASA HABITACIÓN CON VIOLENCIA                       775\n 6       2019 ROBO A CUENTAHABIENTE SALIENDO DEL CAJERO CON VIOLENCIA    696\n 7       2019 ROBO A NEGOCIO CON VIOLENCIA                             12662\n 8       2019 ROBO A PASAJERO A BORDO DE MICROBUS CON Y SIN VIOLENCIA   1282\n 9       2019 ROBO A PASAJERO A BORDO DE TAXI CON VIOLENCIA              416\n10       2019 ROBO A PASAJERO A BORDO DEL METRO CON Y SIN VIOLENCIA     3163\n# ℹ 86 more rows\n\n\n\n\nCódigo\nbase %&gt;%\n    filter(anio_hecho&gt;2018) %&gt;%\n    count(anio_hecho,categoria_delito) %&gt;%\n    ggplot(aes(anio_hecho,n))+\n    geom_col()\n\n\n\n\n\n\n\n\n\n\n\nCódigo\nbase %&gt;%\n    filter(anio_hecho&gt;2018) %&gt;%\n    count(anio_hecho,categoria_delito) %&gt;%\n    ggplot(aes(anio_hecho,n,fill=categoria_delito))+\n    geom_col()+\n    theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nLa opción de hacer un facet crea una gráfica para cada delito a través del tiempo.\n\n\nCódigo\nbase %&gt;%\n    filter(anio_hecho&gt;2018) %&gt;%\n    count(anio_hecho,categoria_delito) %&gt;%\n    ggplot(aes(anio_hecho,n,fill=categoria_delito))+\n    geom_col()+\n    facet_wrap(~categoria_delito,\n        scales = \"free_y\")\n\n\n\n\n\n\n\n\n\n\n\n3.2.2 Hexbins y el algoritmo H3\nA partir de aquí se utiliza el delito de violación, anonimizando posiciones con un mapa de hexbin e indexándolo con el algoritmo H3. Para comparar años, se consideran únicamente del año 2019 al 2023.\n\n\nCódigo\nbase %&gt;%\n    filter(anio_hecho&gt;2018) %&gt;%\n    filter(anio_hecho&lt;2024) %&gt;%\n    count(anio_hecho,categoria_delito) %&gt;%\n    filter(categoria_delito==\"VIOLACIÓN\") %&gt;%\n    ggplot(aes(anio_hecho,n))+\n    geom_col()\n\n\n\n\n\n\n\n\n\nCreamos un objeto que contiene sólo esta información, quedándonos únicamente con los datos que sí tienen latitud y longitud (todo se puede hacer en un solo filter).\n\n\nCódigo\ndelito = base %&gt;%\n    filter(anio_hecho&gt;2018,\n        anio_hecho&lt;2024,\n        categoria_delito==\"VIOLACIÓN\",\n        !is.na(longitud),\n        !is.na(latitud)\n        )\n\n\nUn heatmap indica dónde se concentran los eventos sin decir claramente la relación de magnitudes entre zonas. Los parámetros de addHexbin permiten indicar la escala de colores a utilizar y el tamaño de éstos, siendo 10 para indicar que todos los hexágonos sean del mismo tamaño.\n\n\nCódigo\nlibrary(leaflet.extras2)\n\nleaflet() %&gt;%\n    addProviderTiles(providers$Esri) %&gt;%\n    addHexbin(data=delito,\n        lng =~longitud,\n        lat =~latitud,\n        options = hexbinOptions(radiusRange = 10,\n        colorRange = c('#fee8c8',\n        '#fdbb84',\n        '#e34a33')),\n\n        opacity = 1\n        )\n\n\n\n\n\n\nEn el hexbin no hay una geometría tal cual, por lo que no permite hacer comparaciones. El algoritmo H3 (Uber’s Hexagonal Hierarchical Spatial Index) ofrece un sistema geoespacial completo que permite consideraciones jerárquicas, por lo que cada punto cae en una zona y en esa zona podemos cuantificar. Cuenta con 16 niveles: del 0 al 15; para los cuales va decreciendo el tamaño del hexágono. Una manera de trabajar con esta herramienta es conjuntando las bibliotecas sf y h3jsr.\nPara el ejemplo, se utilizan tres columnas y se agrega una (con fines de ejemplo) con el valor de 50.\n\n\nCódigo\nlibrary(sf)\nlibrary(h3jsr)\n\ndelito %&gt;%\n    select(anio_hecho,longitud,latitud) %&gt;%\n    mutate(nueva_columna=50)\n\n\n# A tibble: 7,845 × 4\n   anio_hecho longitud latitud nueva_columna\n        &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;\n 1       2019    -99.1    19.3            50\n 2       2019    -99.1    19.5            50\n 3       2019    -99.3    19.3            50\n 4       2019    -99.1    19.3            50\n 5       2019    -99.2    19.3            50\n 6       2019    -99.2    19.4            50\n 7       2019    -99.3    19.3            50\n 8       2019    -99.1    19.3            50\n 9       2019    -99.1    19.5            50\n10       2019    -99.1    19.5            50\n# ℹ 7,835 more rows\n\n\nAhora se agrega el índice geoespacial H3 en el que cae cada punto.\n\n\nCódigo\ndelito %&gt;%\n    select(anio_hecho,longitud,latitud) %&gt;%\n    mutate(nueva_columna=50) %&gt;%\n    st_as_sf(coords = c(\"longitud\",\"latitud\"),crs=4326) %&gt;%\n    mutate(id_h3 =point_to_cell(geometry,res=7))\n\n\nSimple feature collection with 7845 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -99.34075 ymin: 19.13327 xmax: -98.95401 ymax: 19.57738\nGeodetic CRS:  WGS 84\n# A tibble: 7,845 × 4\n   anio_hecho nueva_columna             geometry id_h3          \n *      &lt;dbl&gt;         &lt;dbl&gt;          &lt;POINT [°]&gt; &lt;chr&gt;          \n 1       2019            50  (-99.14254 19.3092) 874995842ffffff\n 2       2019            50  (-99.0709 19.48508) 874995b93ffffff\n 3       2019            50 (-99.26158 19.32718) 874995b30ffffff\n 4       2019            50 (-99.07577 19.31761) 874995852ffffff\n 5       2019            50 (-99.24413 19.31978) 874995b36ffffff\n 6       2019            50 (-99.21911 19.38127) 874995b16ffffff\n 7       2019            50  (-99.29367 19.3382) 874995b35ffffff\n 8       2019            50 (-99.06553 19.31359) 874995852ffffff\n 9       2019            50 (-99.13544 19.49277) 874995b9dffffff\n10       2019            50  (-99.0764 19.49675) 874995b93ffffff\n# ℹ 7,835 more rows\n\n\nLo anterior agregó la columna id_h3, la cual permite visualizar el idhex en el que se cae. Con esto se puede realizar el conteo básico para conocer la distribución o dar un seguimiento temporal. También notemos que se usó una transformación a coordenadas a un objeto espacial usando st_as_sf(), lo cual viene del paquete sf. El argumento coords=c(\"longitud\",\"latitud\") indica qué columnas contienen las coordenadas, crs=4326 especifica el sistema de coordenadas WGS84 (el estándar para GPS y mapas web), y con ello asigna cada punto a una celda hexagonal usando point_to_cell() que lo almacena en la columna id_h3. La función as_tibble() hace que se tenga un formato de tabla ya que con el idhex se prescinde de la columna de geometry.\n\n\nCódigo\ndelito %&gt;%\n    select(anio_hecho,longitud,latitud) %&gt;%\n    mutate(nueva_columna=50) %&gt;%\n    st_as_sf(coords = c(\"longitud\",\"latitud\"),crs=4326) %&gt;%\n    mutate(id_h3 =point_to_cell(geometry,res=8)) %&gt;%\n    as_tibble() %&gt;%\n    select(-geometry)\n\n\n# A tibble: 7,845 × 3\n   anio_hecho nueva_columna id_h3          \n        &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;          \n 1       2019            50 8849958423fffff\n 2       2019            50 884995b931fffff\n 3       2019            50 884995b30dfffff\n 4       2019            50 8849958ec9fffff\n 5       2019            50 884995b36dfffff\n 6       2019            50 884995b161fffff\n 7       2019            50 884995b353fffff\n 8       2019            50 8849958e1bfffff\n 9       2019            50 884995b837fffff\n10       2019            50 88499516d9fffff\n# ℹ 7,835 more rows\n\n\nEl conteo por hexágono es el siguiente.\n\n\nCódigo\ndelito %&gt;%\n    select(anio_hecho,longitud,latitud) %&gt;%\n    mutate(nueva_columna=50) %&gt;%\n    st_as_sf(coords = c(\"longitud\",\"latitud\"),crs=4326) %&gt;%\n    mutate(id_h3 =point_to_cell(geometry,res=7)) %&gt;%\n    as_tibble() %&gt;%\n    select(-geometry) %&gt;%\n    count(id_h3)\n\n\n# A tibble: 185 × 2\n   id_h3               n\n   &lt;chr&gt;           &lt;int&gt;\n 1 874986492ffffff     5\n 2 87498649affffff    11\n 3 87498649bffffff     1\n 4 87499516dffffff    10\n 5 874995800ffffff    27\n 6 874995801ffffff     9\n 7 874995802ffffff    12\n 8 874995804ffffff     4\n 9 874995805ffffff     7\n10 874995806ffffff    12\n# ℹ 175 more rows\n\n\nFinalmente, con lo anterior se crea el data frame que graficaremos.\n\n\nCódigo\ndata_set_con_h3 = delito %&gt;%\n    select(anio_hecho,longitud,latitud) %&gt;%\n    mutate(nueva_columna=50) %&gt;%\n    st_as_sf(coords = c(\"longitud\",\"latitud\"),crs=4326) %&gt;%\n    mutate(id_h3 =point_to_cell(geometry,res=7)) %&gt;%\n    as_tibble() %&gt;%\n    select(-geometry) %&gt;%\n    count(id_h3) %&gt;%\n    mutate(geometry=h3jsr::cell_to_polygon(id_h3)) %&gt;%\n    sf::st_as_sf()\n\n\n\nVisualización\nPara ver, fuera de un mapa, el H3, la función de ggplot llamada geom_sf permite graficar objetos creados con sf.\n\n\nCódigo\nlibrary(viridis)\n\nggplot()+\n    geom_sf(data=data_set_con_h3,aes(fill=n))+\n    viridis::scale_fill_viridis()+\n    theme_bw()\n\n\n\n\n\n\n\n\n\nTambién es posible ver cómo está distribuida la intensidad de los hexágonos. Notemos que los colores se corresponden.\n\n\nCódigo\ndata_set_con_h3 %&gt;%\n    ggplot(aes(n))+\n    geom_histogram(aes(fill=..x..))+\n    theme_bw()+\n    viridis::scale_fill_viridis()\n\n\n\n\n\n\n\n\n\nPero nos gusta verlo sobre el mapa… Como los hexágonos cambian el objeto de punto a polígono, se debe indicar esto al crear el objeto de leaflet. Es necesario indicar: la paleta de colores (por consistencia se usa viridis) y la variable a la que se aplica el valor con domain.\n\n\nCódigo\ncolores = colorNumeric(\n    palette = \"viridis\",\n    domain = data_set_con_h3$n\n    )\n\nleaflet() %&gt;%\n    addProviderTiles(providers$OpenStreetMap) %&gt;%\n    addPolygons(data=data_set_con_h3,\n    weight = 1,\n    fillColor = ~colores(n),\n    fillOpacity = 1\n    )\n\n\n\n\n\n\n\n\n\n\nLovelace, Robin, Jakub Nowosad, y Jannes Muenchow. 2025. Geocomputation with R. First. CRC Press. https://r.geocompx.org/.\n\n\nWickham, Hadley, y Garrett Grolemund. 2017. R for Data Science. First. O’Reilly Media, Inc. https://r4ds.had.co.nz/.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Visualización de datos espaciales</span>"
    ]
  },
  {
    "objectID": "Caps/03-Poisson.html",
    "href": "Caps/03-Poisson.html",
    "title": "4  Procesos de Poisson",
    "section": "",
    "text": "4.1 Proceso Poisson homogéneo\nEn este capítulo se presenta uno de los modelos básicos para el tratamiento de datos puntuales: el proceso Poisson. Para motivar su uso en problemas espaciales, primero recordaremos la génesis de éste en el contexto de los procesos en el tiempo.\nUno de los objetivos principales en el análisis de datos puntuales, es caracterizar conteos de eventos que ocurren de manera (presuntamente) aleatoria. Primero recordemos qué es un proceso de conteo.\nHay diversas variables aleatorias de conteo, pero la ley de eventos raros de Poisson (ver por ejemplo, (Klenke 2020)), sugiere que para el conteo de eventos con probabilidad ‘’baja’’ de ocurrir, es razonable utilizar variables aleatorias Poisson.\nExisten definiciones del proceso Poisson homogéneo equivalentes a Definición 4.2, pero ésta es de particular utilidad porque nos proporciona una manera de calcular explícitamente las distribuciones finito dimensionales de un proceso Poisson homogéneo y además éstas únicamente dependen de la longitud de los incrementos.\nCon estas propiedades, tenemos el siguiente estimador de máxima verosimilitud para el parámetro de intensidad \\(\\lambda\\).\nEl valor del estimador es intuitivo ya que dice que la intensidad estimada es el número de eventos por unidad de tiempo, lo cual es consistente con la intuición tras la palabra tasa. Además, se puede verificar que este es un estimador consistente e insesgado que además alcanza la cota de Cramér-Rao.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Procesos de Poisson</span>"
    ]
  },
  {
    "objectID": "Caps/03-Poisson.html#proceso-poisson-homogéneo",
    "href": "Caps/03-Poisson.html#proceso-poisson-homogéneo",
    "title": "4  Procesos de Poisson",
    "section": "",
    "text": "Definición 4.1 (Proceso de conteo) Sea \\(N={(N_t)}_{t\\geq 0}\\) un proceso estocástico. \\(N\\) es un proceso de conteo si toma valores en \\(\\mathbb{N}\\cup\\{0\\}\\) y si \\(s&lt;t\\), entonces, \\(N_s\\leq N_t\\).\n\n\n\nDefinición 4.2 (Proceso Poisson homogéneo) Un proceso de conteo \\(N={(N_t)}_{t\\geq 0}\\) es un proceso Poisson homogéneo de intensidad \\(\\lambda&gt;0\\) si cumple que\n\n\\(N_0=0\\), casi seguramente.\nTiene incrementos independientes y estacionarios, casi seguramente.\nPara \\(s&lt;t\\), un incremento es tal que \\((N_t-N_s)\\sim\\text{Poi}(\\lambda(t-s))\\).\n\n\n\n\n\nTeorema 4.1 (Distribución conjunta y estimador máximo verosímil) Sea \\(N={(N_t)}_{t\\geq 0}\\) un proceso Poisson homogéneo con intensidad \\(\\lambda&gt;0\\) observado en el intervalo \\((0,T_0]\\). Supongamos que se registran \\(n\\) eventos en los tiempos \\(0&lt;w_1&lt;w_2&lt;\\cdots&lt;w_n\\leq T_0\\). Entonces\n\nDado que \\(N_{T_0}=n\\), la densidad conjunta de los tiempos de ocurrencia es\n\n\\[f_{W_1,\\ldots,W_n|N_{T_0}}(w_1,\\ldots,w_n|n)=\\frac{n!}{T_0^n},\\qquad\\text{para}\\ 0\\leq w_1&lt;\\cdots&lt;w_n\\leq T_0.\\]\n\nLa función de verosimilitud para \\(\\lambda\\) es\n\n\\[L(\\lambda;\\underline{x})=\\lambda^ne^{-\\lambda T_0}.\\]\n\nBajo esta formulación, el estimador máximo verosímil de \\(\\lambda\\) es\n\n\\[\\widehat{\\lambda}=\\frac{N_{T_0}}{T_0}.\\]\n\n\nPrueba. \n\nEsta primera afirmación es un resultado clásico de los procesos de Poisson homogéneos y puede revisarse por ejemplo en (Kingman 2007) o en (Karlin y Taylor 1981).\nComo \\(N\\) es un proceso Poisson homogéneo, la probabilidad de observar exactamente \\(n\\) eventos en el intervalo \\((0,T_0]\\) es\n\n\\[\\mathbb{P}[N_{T_0}=n]=e^{-\\lambda T_0}\\frac{{(\\lambda T_0)}^n}{n!}\\Bbb{1}_{\\mathbb{N}\\cup\\{0\\}}(n).\\]\nEntonces, por el punto anterior, la función de verosimilitud de \\(\\lambda\\) es\n\\[L(\\lambda;\\underline{x})\\propto\\mathbb{P}[N_{T_0}=n]f_{W_1,\\ldots,W_n|N_{T_0}}(w_1,\\ldots,w_n|n)=e^{-\\lambda T_0}\\frac{{(\\lambda T_0)}^n}{n!}\\frac{n!}{T_0^n}=\\lambda^n e^{-\\lambda T_0}.\\]\n\nTomamos la log-verosimilitud a partir del inciso anterior, de modo que\n\n\\[\\ell(\\lambda;\\underline{x})=\\ln L(\\lambda)=n\\ln\\lambda-\\lambda T_0.\\]\nEl punto crítico de esta función lo encontramos como\n\\[\\left.\\frac{d\\ell}{d\\lambda}\\right|_{\\lambda=\\lambda^*}=0\\iff\\frac{n}{\\lambda^*}-T_0=0\\implies\\lambda^*=\\frac{n}{T_0}.\\]\nVerificamos que es un máximo con el criterio de la segunda derivada, ya que\n\\[\\left.\\frac{d^2\\ell}{d\\lambda^2}\\right|_{\\lambda=\\lambda^*}=-\\frac{n}{\\lambda^2}&lt;0.\\]\nDe lo anterior, se concluye que el estimador máximo verosímil es\n\\[\\widehat{\\lambda}=\\frac{N_{T_0}}{T_0}.\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Procesos de Poisson</span>"
    ]
  },
  {
    "objectID": "Caps/03-Poisson.html#proceso-poisson-espacial-homogéneo",
    "href": "Caps/03-Poisson.html#proceso-poisson-espacial-homogéneo",
    "title": "4  Procesos de Poisson",
    "section": "4.2 Proceso Poisson espacial homogéneo",
    "text": "4.2 Proceso Poisson espacial homogéneo\nSiguiendo la idea de que un proceso de Poisson sobre el tiempo cuenta cuántos puntos caen en cada intervalo de \\(\\mathbb{R}_+\\) tras arrojarlos a una intensidad \\(\\lambda&gt;0\\), es posible extender la noción del proceso Poisson a espacios más generales.\n\nDefinición 4.3 (Proceso Poisson espacial homogéneo) Sea \\(S\\subseteq\\mathbb{R}^d\\) un conjunto Lebesgue-medible y \\(\\mathcal{A}\\) una familia de subconjuntos Lebesgue-medibles de \\(S\\) cerrada bajo uniones e intersecciones finitas. Decimos que un proceso espacial \\(N={(N(A))}_{A\\in\\mathcal{A}}\\) es un proceso Poisson homogéneo de intensidad \\(\\lambda&gt;0\\) en la región \\(S\\) si - Para todo \\(A\\in\\mathcal{A}\\) se tiene que \\(N(A)\\sim\\text{Poi}(\\lambda|A|)\\), donde \\(|A|\\) representa la medida de Lebesgue del conjunto \\(A\\). - Para todos \\(A_1,\\ldots,A_n\\in\\mathcal{A}\\) disjuntos, las variables aleatorias \\(N(A_1),\\ldots,N(A_n)\\) son independientes. - Dado que \\(N(A)=n\\), las ubicaciones \\(X_1,\\ldots,X_n\\) de los eventos son uniformes en \\(A\\), es decir\n\\[f_{X_1,\\ldots,X_n|N(A)}(x_1,\\ldots,x_n|n)=\\frac{1}{{|A|}^n}.\\]\n\nAsí, un proceso Poisson espacial homogéneo es un proceso donde la intensidad con la que se arrojan puntos a una región \\(A\\) del espacio, es proporcional al tamaño de éste. Esto hace sentido con la idea intuitiva de que ‘’es más probable que contemos más puntos si el área es más grande’’.\nEn analogía, metodológica e intuitivamente, al caso temporal, tenemos el siguiente estimador para la intensidad de un proceso Poisson espacial homogéneo.\n\nTeorema 4.2 (Estimador máximo verosímil) Sea \\(N={(N(A))}_{A\\in\\mathcal{A}}\\) un proceso Poisson espacial homogéneo con intensidad \\(\\lambda&gt;0\\) observado en la región \\(A_0\\), de tamaño \\(|A_0|\\). Supongamos que se registran \\(n\\) eventos en las posiciones \\(x_1,x_2,\\ldots,x_n\\in A_0\\). Entonces el estimador máximo verosímil de \\(\\lambda\\) es\n\\[\\widehat{\\lambda}=\\frac{N(A_0)}{|A_0|}.\\]\n\n\nPrueba. Siguiendo la idea del caso temporal, la función de verosimilitud de \\(\\lambda\\) se puede calcular con las distribuciones conocidas como\n\\[\n\\begin{align*}\nL(\\lambda;\\underline{x})&\\propto f_{X_1,\\ldots,X_n|N(A_0)}(x_1,\\ldots,x_n)\\mathbb{P}[N(A_0)=n]\\\\\n&=e^{-\\lambda|A_0|}\\frac{{(\\lambda|A_0|)}^n}{n!}\\frac{1}{{|A_0|}^n}=\\frac{\\lambda^n}{n!}e^{-\\lambda|A|}.\n\\end{align*}\n\\]\nLa log-verosimilitud es\n\\[\\ell(\\lambda;\\underline{x})=n\\ln\\lambda-\\lambda|A_0|\\]\nEl punto crítico de esta función lo encontramos como\n\\[\\left.\\frac{d\\ell}{d\\lambda}\\right|_{\\lambda=\\lambda^*}=0\\iff\\frac{n}{\\lambda^*}-|A_0|=0\\implies\\lambda^*=\\frac{n}{|A_0|}.\\]\nVerificamos que es un máximo con el criterio de la segunda derivada, ya que\n\\[\\left.\\frac{d^2\\ell}{d\\lambda^2}\\right|_{\\lambda=\\lambda^*}=-\\frac{n}{\\lambda^2}&lt;0.\\]\nDe lo anterior, se concluye que el estimador máximo verosímil es\n\\[\\widehat{\\lambda}=\\frac{N(A_0)}{|A_0|}.\\]\n\nEl valor del estimador es intuitivo ya que dice que la intensidad estimada es el número de eventos por unidad de volumen, lo cual es consistente con la intuición tras la palabra tasa. Además, se puede verificar que este es un estimador consistente e insesgado que además alcanza la cota de Cramér-Rao.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Procesos de Poisson</span>"
    ]
  },
  {
    "objectID": "Caps/03-Poisson.html#proceso-poisson-no-homogéneo",
    "href": "Caps/03-Poisson.html#proceso-poisson-no-homogéneo",
    "title": "4  Procesos de Poisson",
    "section": "4.3 Proceso Poisson no homogéneo",
    "text": "4.3 Proceso Poisson no homogéneo\nRegresamos al contexto temporal para motivar el siguiente modelo. Una de las principales debilidades del proceso Poisson homogéneo viene de la homogeneidad. El asumir que la intensidad es constante puede llevar a ideas como que los accidentes de tránsito ocurren a la misma tasa en la madrugada que en la tarde, y eso no es una hipótesis razonable, en general. La manera más inmediata de resolver esto es considerando que la intensidad ahora también varía en el tiempo.\n\nDefinición 4.4 (Proceso Poisson no homogéneo) Sea \\(\\lambda:[0,\\infty)\\to[0,\\infty)\\) una función tal que\n\\[\\int_0^\\infty\\lambda(t)dt=\\infty.\\]\nUn proceso de contar \\(N={(N_t)}_{t\\geq 0}\\) es un proceso Poisson no homogéneo con función de intensidad \\(\\lambda(t)\\) si satisface que\n\n\\(N_0=0\\) casi seguramente.\nTiene incrementos independientes.\n\\(\\mathbb{P}[N_{t+h}-N_t=1]=\\lambda(t)h+o(h)\\).\n\\(\\mathbb{P}[N_{t+h}-N_t\\geq 2]=o(h)\\).\n\n\nComo consecuencia de esta formulación infinitesimal, se tiene el siguiente resultado, el cual da la función de densidad de los incrementos de un proceso Poisson no homogéneo.\n\nTeorema 4.3 (Densidad de los incrementos) Sea \\(N={(N_t)}_{t\\geq 0}\\) un proceso Poisson no homogéneo con función de intensidad \\(\\lambda\\). Entonces, para cada \\(n\\geq 0\\) y \\(0\\leq s&lt;t\\), se tiene que\n\\[\\mathbb{P}[N_{t+s}-N_t=n]=\\exp\\left[-(\\Lambda(t+s)-\\Lambda(t))\\right]\\frac{{(\\Lambda(t+s)-\\Lambda(t))}^n}{n!},\\]\ndonde\n\\[\\Lambda(t)=\\int_0^t\\lambda(u)du\\]\nes la intensidad acumulada. Más aún, dado que han ocurrido \\(n\\) eventos en el intervalo \\((0,T]\\), los tiempos de ocurrencia de éstos son tales que\n\\[f_{W_1,\\ldots,W_n|N_t}(w_1,\\ldots,w_n|n)\\propto\\frac{1}{{(\\Lambda(t))}^n}\\prod_{i=1}^n\\lambda(w_i).\\]\n\nEs importante notar que ahora se busca estimar una función. La función de verosimilitud de este problema es\n\\[L(\\lambda(t))=\\left[\\prod_{i=1}^n\\lambda(t_i)\\right]\\exp\\left(-\\Lambda(T)\\right).\\]\nLos métodos utilizados para estos problemas, son métodos para estimación de funciones. Hay distintas maneras de abordar este problema. El problema es que, como la función \\(\\lambda\\) no es propiamente una densidad, es necesario trabajar con alguna función relacionada que sí lo sea. Si consideramos la función \\(f:[0,T]\\to\\mathbb{R}\\) dada por\n\\[f(t)=\\frac{\\lambda(t)}{\\Lambda(T)},\\]\nahora sí tenemos una función de densidad en el intervalo \\([0,T]\\). Hay muchas maneras de resolver este problema de estimación de funciones. La manera más directa y como secuencia natural al Curso de Modelos Estadísticos, más específicamente al tema de regresión Poisson. Entonces, utilizando una función de enlace \\(\\log\\), se tiene el modelo Log-Lineal\n\\[\\log\\lambda(t)=\\beta_0+\\beta_1Z_1(t)+\\cdots+\\beta_pZ_p(t),\\]\ndonde \\(Z_1,\\ldots,Z_p\\) son covariables temporales y \\(\\beta_0,\\ldots,\\beta_p\\) son los coeficientes a estimar (podría verse como un problema de estimación de tendencia y estacionalidad de una serie de tiempo, por ejemplo). Otra manera sería considerar la versión empírica de \\(\\lambda\\) que se tiene por los tiempos \\(t_1,\\ldots,t_n\\) y utilizar algún método de suavizamiento por kernel de modo que se tendría\n\\[\\widehat{\\lambda}(t)=\\sum_{i=1}^n\\frac{1}{h}\\kappa\\left(\\frac{t-t_i}{h}\\right),\\]\npara \\(\\kappa\\) un kernel Gaussiano y un ancho de banda \\(h\\).\nUna manera de validar modelos de procesos Poisson no homogéneos y también de simularlos es el siguiente, que relaciona un proceso Poisson homogéneo con uno no homogéneo a partir de un cambio de tiempo determinista.\n\nTeorema 4.4 (Relación entre procesos Poisson homogéneos y no homogéneos)  \n\nSea \\({(N_t)}_{t\\geq 0}\\) un proceso Poisson no homogéneo con funciones de intensidad \\(\\lambda\\) y de intensidad acumulada \\(\\Lambda\\), con inversa generalizada \\(\\Lambda^{-1}\\). Entonces el proceso \\({(N_{\\Lambda^{-1}(t)})}_{t\\geq 0}\\) es un proceso Poisson homogéneo de intensidad \\(\\lambda=1\\).\nSea \\({(N_t)}_{t\\geq 0}\\) un proceso Poisson homogéneo de intensidad \\(\\lambda=1\\) y sea \\(\\lambda:[0,\\infty)\\to[0,\\infty)\\) una función no negativa tal que \\(\\displaystyle\\int_0^\\infty\\lambda(t)dt=\\infty\\). Definimos la función \\(\\Lambda\\) como \\(\\displaystyle\\Lambda(t)=\\int_0^t\\lambda(u)du\\). Entonces el proceso \\({(N_{\\Lambda(t)})}_{t\\geq 0}\\) es un proceso Poisson no homogéneo con función de intensidad \\(\\lambda\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Procesos de Poisson</span>"
    ]
  },
  {
    "objectID": "Caps/03-Poisson.html#proceso-poisson-espacial-no-homogéneo",
    "href": "Caps/03-Poisson.html#proceso-poisson-espacial-no-homogéneo",
    "title": "4  Procesos de Poisson",
    "section": "4.4 Proceso Poisson espacial no homogéneo",
    "text": "4.4 Proceso Poisson espacial no homogéneo\nCon las ideas de la sección anterior, un proceso Poisson espacial no homogéneo, intuitivamente, es uno en que la intensidad depende del punto del espacio en el que uno se encuentre. Teóricamente, la extensión no es tan directa como se pensaría en primera instancia.\n\nDefinición 4.5 (Proceso Poisson espacial no homogéneo) Sea \\(S\\subseteq\\mathbb{R}^d\\) un conjunto Lebesgue-medible y \\(\\mathcal{A}\\) una familia de subconjuntos Lebesgue-medibles de \\(S\\) cerrada bajo uniones e intersecciones finitas. Decimos que un proceso espacial \\(N={(N(A))}_{A\\in\\mathcal{A}}\\) es un proceso Poisson no homogéneo con función de intensidad \\(\\lambda:S\\to[0,\\infty)\\) en la región \\(S\\) si\n\nPara todo \\(A\\in\\mathcal{A}\\) se tiene que \\(N(A)\\sim\\text{Poi}(\\Lambda(A))\\), donde\n\n\\[\\Lambda(A)=\\int_A\\lambda(x)dx.\\]\n\nPara todos \\(A_1,\\ldots,A_n\\in\\mathcal{A}\\) disjuntos, las variables aleatorias \\(N(A_1),\\ldots,N(A_n)\\) son independientes.\nDado que \\(N(A)=n\\), las ubicaciones \\(X_1,\\ldots,X_n\\) de los eventos son tales que\n\n\\[ f_{X_1,\\ldots,X_n|N(A)}(x_1,\\ldots,x_n|n)=\\frac{1}{{(\\Lambda(A))}^n}\\prod_{i=1}^n\\lambda(x_i). \\]\n\nEn este caso, la función de intensidad en realidad se toma como densidad para una medida, a la cual se le llama también medida de Poisson. Detalles teóricos sobre estos procesos de Poisson con intensidades \\(\\lambda\\) generales se pueden revisar en (Kingman 2007).\n\nEjemplo 4.1 Sean \\(S\\subseteq\\mathbb{R}^2\\) y \\(\\lambda(x,y)=e^{-(x^2+y^2)}\\). Entonces, si \\(A=\\{(x,y)\\ :\\ x^2+y^2\\leq r^2\\}\\), la medida de \\(A\\) respecto a \\(\\lambda\\) es\n\\[\\Lambda(A)=\\int_A e^{-(x^2+y^2)}dxdy=\\pi(1-e^{-r^2}).\\]\n\nLa manera en la que se puede hacer inferencia sobre la función \\(\\lambda\\) es la misma que en el caso temporal. La función de verosimilitud sobre la región \\(A\\) de este modelo es\n\\[L(\\lambda(t))=\\left[\\prod_{i=1}^n\\lambda(x_i)\\right]\\exp(-\\Lambda(A)).\\]\nEl modelo Log-Lineal en este caso es\n\\[\\log\\lambda(s)=\\beta_0(s)+\\beta_1Z(s).\\]\nUna hipótesis usual es considerar que \\(Z={(Z(s))}_{s\\in A}\\) es un campo Gaussiano de media 0. Dos métodos clásicos para la estimación son los siguientes:\n\nCon una discretización del espacio en una rejilla de \\(n_1\\times n_2=N\\) celdas \\({(s_{ij})}_{(i,j)\\in\\{1,\\ldots,n_1\\}\\times\\{1,\\ldots,n_2\\}}\\). El número promedio de eventos por celda es\n\n\\[\\Lambda_{ij}=\\int_{s_{ij}}\\exp(\\eta(s))ds\\approx|s_{ij}|\\exp(\\eta_{ij}).\\]\nEntonces, dado un campo latente \\(\\eta_{ij}\\), el número de observaciones en la rejilla es tal que\n\\[(y_{ij}|\\eta_{ij})\\sim\\text{Poi}(|s_{ij}|\\exp(\\eta_{ij})),\\]\nEl campo latente puede estimarse a partir de otras covariables y efectos aleatorios como\n\\[\\eta_{ij}=c(s_{ij})\\beta+f_s(s_{ij})+f_u(s_{ij}),\\]\ndonde \\(\\beta=(\\beta_0,\\beta_1,\\ldots,\\beta_p),\\ c(s_{ij})=(1,c_1(s_{ij}),\\ldots,c_p(s_{ij})),\\ f_s\\) es un ruido dependiente de la estructura espacial y \\(f_u\\) es un ruido no necesariamente dependiente del espacio.\n\nCon un proceso análogo al autorregresivo de las series de tiempo\n\n\\[Z(s)=\\sum_{i=1}^nZ_i\\phi_i(s),\\]\ndonde \\((Z_1,\\ldots,Z_n)\\) es un vector Gaussiano y \\({(\\phi_i(s))}_{i=1}^n\\) es una base de funciones deterministas. Una alternativa usual es considerar que el campo Gaussiano tiene matriz de covarianza como en el variograma de Mathéron presentado en la sección Sección 2.2.2.\nEste modelo no se limita únicamente a que tome valores de puntos en el espacio. La definición contempla casos en los que la función de intensidad sea de la forma \\(\\lambda:[0,\\infty)\\times\\mathbb{R}^d\\to\\mathbb{R}_+\\) como \\(\\lambda=\\lambda(t,x)\\), es decir, que la intensidad dependa del instante y de la ubicación. Un problema para ilustrar esta forma de \\(\\lambda\\) es el de las lluvias, que su intensidad puede depender de la hora del día y del lugar del mundo del que se trate. Más aún, podría ser que la función de intensidad dependiera del espacio pero sea más fácil expresar esa dependencia a partir de covariables: para medir propensión a lluvia, podría ser que de interés primero conocer la temperatura \\(T=T(x,y)\\), la humedad \\(H=H(x,y)\\), los niveles de contaminantes en el aire \\(C=C(x,y)\\), la hora del día \\(t\\), entre otros posibles factores. Este tipo de covariables pueden incluirse en la estructura de regresión del modelo Log-Lineal.\nDe la misma manera que con el análisis de procesos temporales, es posible usar suavizamiento por kernel para estimar \\(\\lambda\\) a partir de una medida empírica. Como se mencionó en el caso temporal, la función \\(\\lambda\\) no necesariamente es una densidad, pero la función\n\\[f(x)=\\frac{1}{\\Lambda(A)}\\lambda(x)\\]\nsí es una densidad en la región \\(A\\). Entonces, los estimadores de kernel para \\(f\\) y \\(\\lambda\\) basados en un conjunto de observaciones \\(X_1,\\ldots,X_n\\) están dados por\n\\[\n\\begin{align*}\n\\widehat{f}(x)&=\\frac{1}{n}\\sum_{i=1}^n\\frac{1}{h^2}\\kappa\\left(\\frac{x-x_i}{h}\\right),\\\\\n\\widehat{\\lambda}(x)&=\\sum_{i=1}^n\\frac{1}{h^2}\\kappa\\left(\\frac{x-x_i}{h}\\right),\n\\end{align*}\n\\]\ndonde \\(h\\) es un ancho de banda. Una elección usual para \\(\\kappa\\) es la normal multivariada.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Procesos de Poisson</span>"
    ]
  },
  {
    "objectID": "Caps/03-Poisson.html#pruebas-de-clústering",
    "href": "Caps/03-Poisson.html#pruebas-de-clústering",
    "title": "4  Procesos de Poisson",
    "section": "4.5 Pruebas de clústering",
    "text": "4.5 Pruebas de clústering\nLa última propiedad enunciada en la definición Definición 4.3 hace que al proceso Poisson espacial homogéneo también se le llame el modelo de aleatoriedad espacial completa o CSR por sus siglas en inglés (complete spatial randomness). Si bien la mayoría de procesos están lejos de ser CSR, éste ayuda a diferenciar entre patrones regulares y conglomerados (o clusterizados).\nDada una muestra de puntos, un buen primer paso es preguntarse si hay evidencia para rechazar la hipótesis de que se trate de un CSR.\n\n4.5.1 Prueba con quadrat\nUn método para dar evidencia a favor o en contra de la hipótesis nula \\(H_0\\) ser CSR es con una prueba \\(\\chi^2\\) (ver (Moraga 2023)). El método quadrat coincide en particionar la región de estudio en \\(r\\) renglones y \\(c\\) columnas, de modo que se obtenga una retícula de regiones disjuntas de la misma área (quadrats). Así, bajo \\(H_0\\), el número esperado de observaciones en cualquier región de la retícula es el mismo: sean \\(n\\) el número de observaciones, \\(m\\) el número de quadrats y \\(n_i\\) el número de puntos en el \\(i\\)-ésimo quadrat; entonces el número esperado de puntos en cada quadrat es \\(n^*=n/m\\).\nDado lo anterior, el estadístico de prueba se puede calcular por\n\\[X^2=\\sum_{i=1}^m\\frac{{(n_i-n^*)}^2}{n^*}.\\]\nSe puede verificar que, bajo \\(H_0\\), entonces \\(X^2\\sim\\chi^2_{m-1}\\).\nEs muy importante destacar que este método depende de la configuración de quadrats (puede haber regiones sin puntos). También esta prueba no puede distinguir entre distintos patrones localmente.\nEn R se tiene la función quadrat.test() para probar la hipótesis de CSR. La interpretación es de acuerdo a la siguiente clave:\n\nalternative = “two.sided” prueba \\(H_0:\\) CSR vs. \\(H_a:\\) no CSR (regular o clusterizada).\nalternative = “regular” prueba \\(H_0:\\) CSR o clusterizada vs. \\(H_a:\\) regular.\nalternative = “clustered” prueba \\(H_0:\\) CSR o regular vs. \\(H_a:\\) clusterizada.\n\n\n\n4.5.2 Las funciones K y L de Ripley\nPensando en datos puntuales sobre regiones del espacio \\(S\\subseteq\\mathbb{R}^2\\), se puede usar otro estadístico para probar la homogeneidad del proceso puntual.\n\nDefinición 4.6 (Funciones K y L de Ripley) Sea \\(N={(N(A))}_{A\\in\\mathcal{A}}\\) un proceso Poisson espacial en \\(\\mathbb{R}^2\\) con función de intensidad \\(\\lambda\\).\n\nLa función K de Ripley es la función dada por\n\n\\[K(s)=\\frac{1}{\\lambda}\\mathbb{E}_0[N(B_r(0)\\backslash\\{0\\})],\\]\ndonde \\(\\mathbb{E}_0\\) es la esperanza condicionada a que haya un punto en el origen y \\(B_r(0)\\) es la bola de radio \\(r\\) centrada en el origen.\n\nLa función L de Ripley es la función dada por\n\n\\[L(r)=\\sqrt{\\frac{K(r)}{\\pi}}.\\]\n\n\n\n\nVisualización del cálculo de la función de Ripley empírica.\n\n\nNotemos que bajo la hipótesis de CSR, la función \\(\\lambda\\) es constante y las funciones de Ripley están dadas explícitamente por\n\\[\n\\begin{align*}\nK(r)&=\\frac{1}{\\lambda}\\mathbb{E}_0[N(B_r(0)\\backslash\\{0\\})]=\\frac{1}{\\lambda}\\mathbb{E}_0[N(B_r(0))]\\\\\n&=\\frac{|B_r(0)|\\lambda}{\\lambda}=\\pi r^2,\\\\\nL(r)&=\\sqrt{\\frac{K(r)}{\\pi}}=\\sqrt{\\frac{\\pi r^2}{\\pi}}=r.\n\\end{align*}\n\\]\nDada la forma de la función \\(K\\) bajo CSR, se tiene el siguiente criterio:\n\nSi \\(K(r)&gt;\\pi r^2\\) o, equivalentemente, \\(L(r)&gt;r\\), la función de intensidad favorece el clústering.\nSi \\(K(r)&lt;\\pi r^2\\) o, equivalentemente, \\(L(r)&lt;r\\), la función de intensidad favorece la dispersión.\n\nUna manera de estimar la función \\(K\\) es utilizando que es un conteo. Para hacer éste, es necesario tomar en cuenta que los eventos contados cerca de la frontera de una región \\(A\\) pueden ser bajos. Por ello s eintroduce un peso \\(w_{ij}\\) correspondiente al recíproco a la proporción del círculo con centro en \\(x_i\\) y radio \\(d_{ij}\\) contenido en \\(A\\).\n\nDefinición 4.7 (Estimadores de las funciones de Ripley) Sea \\(X=\\{X_1,\\ldots,X_n\\}\\) un patrón de puntos en \\(S\\). La función \\(K\\) de Ripley empírica está dada por\n\\[\\widehat{K}(r)=\\frac{1}{\\widehat{\\lambda}n}\\sum_{i=1}^n\\sum_{j\\neq i}w_{ij}^{-1}\\Bbb{1}_{\\{d_{ij}\\leq r\\}}.\\]\nBajo la hipótesis de CSR, el estimador de \\(\\lambda\\) es \\(\\displaystyle\\widehat{\\lambda}=\\frac{n-1}{|S|}\\), entonces la función \\(K\\) empírica es\n\\[\\widehat{K}(r)=\\frac{|S|}{n(n-1)}\\sum_{i=1}^n\\sum_{j\\neq i}w_{ij}^{-1}\\Bbb{1}_{\\{d_{ij}\\leq r\\}}.\\]\nDel mismo modo, la la función \\(L\\) de Ripley empírica está dada por\n\\[\\widehat{L}(r)=\\sqrt{\\frac{\\widehat{K}(r)}{\\pi}}.\\]\n\nLos intervalos de confianza para estas funciones empíricas típicamente son intervalos bootstrap. La idea de utilizar las funciones de Ripley es parecida a la de las gráficas QQ en inferencia de distribuciones: entre más cerca esté el proceso de ser homogéneo en el espacio, más parecidas serán las funciones teórica y empírica.\nEn el siguiente ejemplo se muestra un ejemplo del uso de estas funciones con datos simulados directamente de un proceso Poisson homogéneo.\n\nEjemplo 4.2 Primero generamos un proceso Poisson homogéneo de intensidad \\(\\lambda=100\\) y revisamos la región \\(A=[0,1]\\times[0,1]\\).\n\n\nCódigo\nlibrary(spatstat)\n\nset.seed(612000)\nX &lt;- rpoispp(lambda = 100, win = owin(c(0,1), c(0,1)))\n\nplot(X, \n     main = \"Proceso de Poisson Homogéneo\",\n     pch = 20,                      \n     cols = \"blue\",                  \n     show.window = TRUE,             \n     axes = TRUE,                    \n     xlab = \"X\",          \n     ylab = \"Y\"        \n     )\n\n\n\n\n\n\n\n\n\nA partir de la simulación, calculamos el estimador de \\(\\lambda\\) bajo el supuesto de CSR.\n\n\nCódigo\nlambda_hat&lt;- data.frame(\"lambda_hat=\",intensity(X)) \nlambda_hat\n\n\n  X.lambda_hat.. intensity.X.\n1    lambda_hat=           94\n\n\nEste estimador sí es bastante parecido al valor real utilizado. Así, la función \\(K\\) empírica superpuesta a la teórica se ve como\n\n\nCódigo\nK &lt;- Kest(X, correction = \"border\")\nK_env &lt;- envelope(X, Kest, nsim=99)\n\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\nCódigo\nplot(K_env, main = \"Función K con bandas de confianza de 99 simulaciones\")\n\n\n\n\n\n\n\n\n\nO, más claramente con la función \\(L\\), se ve como\n\n\nCódigo\nL&lt;-Lest(X, correction = \"border\")\nL_env &lt;- envelope(X, Lest, nsim=99)\n\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\nCódigo\nplot(L_env, main = \"Función L con bandas de confianza de 99 simulaciones\")\n\n\n\n\n\n\n\n\n\nAl ser las funciones empíricas parecidas a las funciones teóricas, podemos afirmar que los datos trabajados sí corresponden a un CSR.\nPara sustentar esta última afirmación, podemos utilizar la prueba quadrat, la cual también da evidencia a favor de la hipótesis de CSR.\n\n\nCódigo\nquadrat.test(X, nx = 4, ny = 4)\n\n\n\n    Chi-squared test of CSR using quadrat counts\n\ndata:  X\nX2 = 10.17, df = 15, p-value = 0.3822\nalternative hypothesis: two.sided\n\nQuadrats: 4 by 4 grid of tiles\n\n\n\nPara ejemplificar cuándo no ocurre el CSR, tenemos otro ejemplo artificial donde la intensidad es muy alta cerca del \\((0,0)\\).\n\nEjemplo 4.3 Primero generamos un proceso Poisson no homogéneo de intensidad \\(\\displaystyle\\lambda(x,y)=100{(x^2+y^2)}^{-1/2}\\Bbb{1}_{\\{x^2+y^2&gt;0\\}}\\) y revisamos la región \\(A=[0,1]\\times[0,1]\\).\n\n\nCódigo\nlibrary(spatstat)\n\nlambda &lt;- function(x, y) { ifelse(x == 0 & y == 0, 0, 100 / sqrt(x^2 + y^2)) }\nx &lt;- seq(0.01, 1, length.out = 100)\ny &lt;- seq(0.01, 1, length.out = 100)\ngrid &lt;- expand.grid(x = x, y = y)\ngrid$lambda &lt;- lambda(grid$x, grid$y)\nlambda_im &lt;- as.im(data.frame(x = grid$x, y = grid$y, z = grid$lambda), \n                   W = owin(c(0,1), c(0,1)))\n\nset.seed(612003)\nX_nhpp &lt;- rpoispp(lambda_im)\n\nplot(X_nhpp, \n     main = \"Proceso de Poisson no Homogéneo\",\n     pch = 20,                      \n     cols = \"blue\",                  \n     show.window = TRUE,             \n     axes = TRUE,                    \n     xlab = \"X\",          \n     ylab = \"Y\"        \n     )\n\n\n\n\n\n\n\n\n\nAsí, la función \\(K\\) empírica superpuesta a la teórica se ve como\n\n\nCódigo\nK_empirica &lt;- Kest(X_nhpp, correction = \"border\")\n\nK_empenv &lt;- envelope(X_nhpp, Kest, nsim=99)\n\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\nCódigo\nplot(K_empenv, main = \"Función K con bandas de confianza de 99 simulaciones\")\n\n\n\n\n\n\n\n\n\nO, más claramente con la función \\(L\\), se ve como\n\n\nCódigo\nL_empirica &lt;- Lest(X_nhpp, correction = \"border\")\n\nL_empenv &lt;- envelope(X_nhpp, Lest, nsim=99)\n\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\nCódigo\nplot(L_empenv, main = \"Función L con bandas de confianza de 99 simulaciones\")\n\n\n\n\n\n\n\n\n\nAl ser las funciones empíricas muy distintas a las funciones teóricas, podemos afirmar que los datos trabajados no corresponden a un CSR.\nPara sustentar esta última afirmación, podemos utilizar la prueba quadrat, la cual también da evidencia en contra de la hipótesis de CSR.\n\n\nCódigo\nquadrat.test(X_nhpp, nx = 4, ny = 4)\n\n\n\n    Chi-squared test of CSR using quadrat counts\n\ndata:  X_nhpp\nX2 = 146.13, df = 15, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\nQuadrats: 4 by 4 grid of tiles\n\n\n\n\n\n\n\nKarlin, Samuel, y Howard Taylor. 1981. A Second Course in Stochastic Processes. First. Academic Press.\n\n\nKingman, John. 2007. Poisson Processes. Second. Claredon Press.\n\n\nKlenke, Achim. 2020. Probability Theory: A Comprehensive Course. Third. Wiley.\n\n\nMoraga, Paula. 2023. Spatial Statistics for Data Science: Theory and Practice with R. First. Chapman & Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Procesos de Poisson</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Baddeley, Adrian, Ege Rubak, and Thomas Turner. 2015. Spatial Point\nPatterns: Methodology and Applications with r. First. Chapman;\nHall/CRC.\n\n\nBivand, Roger, Edzer Pebesma, and Virgilio Gomez-Rubio. 2013.\nApplied Spatial Data Analysis with r. Second. Springer.\n\n\nCressie, Noel. 2015. Statistics for Spatial Data. Revised.\nWiley.\n\n\nKarlin, Samuel, and Howard Taylor. 1981. A Second Course in\nStochastic Processes. First. Academic Press.\n\n\nKingman, John. 2007. Poisson Processes. Second. Claredon Press.\n\n\nKlenke, Achim. 2020. Probability Theory: A Comprehensive\nCourse. Third. Wiley.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2025.\nGeocomputation with r. First. CRC Press. https://r.geocompx.org/.\n\n\nMoraga, Paula. 2023. Spatial Statistics for Data Science: Theory and\nPractice with r. First. Chapman & Hall/CRC.\n\n\nPebesma, Edzer. 2022a. Simple Features for r. R-project.\n\n\nRincón, Luis, and Verónica De Jesús. 2025. Una Introducción a Las\nSeries de Tiempo. First. La prensa de Ciencias.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data\nScience. First. O’Reilly Media, Inc. https://r4ds.had.co.nz/.",
    "crumbs": [
      "References"
    ]
  }
]